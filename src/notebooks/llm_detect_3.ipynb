{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLytI65hYIQD",
        "outputId": "d96456a1-4411-477d-90e0-74923db6c5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 15 14:27:24 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkHXdgjjg82h",
        "outputId": "f2487b63-f9c6-46c0-c1f8-d54842022cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.33.2\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers==4.33.2\n",
        "!pip3 install optimum==1.13.2\n",
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4gw-wd1g9sK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O00St_JBhAJs"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = 'TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ'  # \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map='auto',\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"gptq-4bit-32g-actorder_True\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H76qqrTyWJq6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"definitions.pickle\", \"rb\") as file:\n",
        "    definitions = pickle.load(file)\n",
        "with open(\"sentences.pickle\", \"rb\") as file:\n",
        "    sentences = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p_EMpwLyten"
      },
      "outputs": [],
      "source": [
        "definitions[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4cDctccfqqd"
      },
      "outputs": [],
      "source": [
        "sentences[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WamCnoGcyuuP"
      },
      "outputs": [],
      "source": [
        "list(zip(definitions, sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tap32gHBHoA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def generate_examples(definition: str, sentence: str, temperature=0.75, top_p=0.95, top_k=1, max_new_tokens=1024):\n",
        "    prompt = lambda definition, sentence: f'''### User: Du bist kreativ und gewissenhaft. Hier ist eine Definition: {definition} Bilde neue Sätze gemäß dieser Definition. Gib die Sätze in einer Python-Liste aus. Gib sonst nichts aus.\n",
        "    ### Example: {sentence}\n",
        "    ### Assistant:[\"'''\n",
        "\n",
        "    prompt_length = len(prompt(definition, sentence))\n",
        "    output = []\n",
        "    first_output = \"\"\n",
        "    i = 100  # only try 100 times\n",
        "    while (\n",
        "        (not any([c.isalpha() for c in first_output]))\n",
        "        or any([x in first_output for x in {\"Konstruktion\", \"Satz\", \"Überschrift\", \"_\", \":\", \"XY\", \"XP\", \"X \", \"Y \", \"X.\", \"Y.\"}])\n",
        "        or re.search(r\".*\\].*\\[.*\", first_output)\n",
        "    ):\n",
        "        input_ids = tokenizer(prompt(definition, sentence), return_tensors='pt').input_ids.cuda()\n",
        "        output = model.generate(inputs=input_ids, temperature=temperature,\n",
        "                                do_sample=True, top_p=top_p, top_k=top_k,\n",
        "                                max_new_tokens=max_new_tokens)\n",
        "        output = tokenizer.decode(output[0])[prompt_length:].strip()\n",
        "        output = re.findall('\\[.*\\]', output)\n",
        "        if len(output) > 0:\n",
        "            first_output = output[0]\n",
        "        i -= 1\n",
        "        print(i, end=\" \")\n",
        "        if i == 0:\n",
        "            first_output = \"[]\"\n",
        "            break\n",
        "        #print(f\"\\t{output}\")\n",
        "    #print(f\"\\n{output}\")\n",
        "    print()\n",
        "    return first_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFWO-TOPy13D"
      },
      "outputs": [],
      "source": [
        "for shot in range(1, 5):\n",
        "    examples = {}\n",
        "    if os.path.exists(f\"examples_{shot}_shot.pickle\"):\n",
        "        with open(f\"examples_{shot}_shot.pickle\", \"rb\") as file:\n",
        "            examples = pickle.load(file)\n",
        "\n",
        "    for k in tqdm(definitions.keys()):\n",
        "        if k in examples.keys():\n",
        "            continue  # have already generated examples for this construction\n",
        "\n",
        "        definition = definitions[k]\n",
        "        try:\n",
        "            sentence = str(list(sentences[int(k)])[0:shot])  # get some sentences\n",
        "        except KeyError:\n",
        "            print((\"[]\", \"[]\", \"This seems wrong...\"))\n",
        "            examples[k] = (\"[]\", \"[]\")\n",
        "            continue\n",
        "\n",
        "        example = generate_examples(definition, sentence, temperature=0.75,\n",
        "                                    max_new_tokens=1000,\n",
        "                                    top_k=100, top_p=0.99)\n",
        "        #example = re.findall('\\[(\"[^\"]*\"(?:, ?\"[^\"]*\")*)\\]', example)[0]\n",
        "        try:\n",
        "            print((sentence, example))\n",
        "            examples[k] = (sentence, example)  # one is enough\n",
        "        except:\n",
        "            print((sentence, \"[]\"))\n",
        "            examples[k] = (sentence, \"[]\")\n",
        "\n",
        "        with open(f\"examples_{shot}_shot.pickle\", \"wb\") as file:\n",
        "            pickle.dump(examples, file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}