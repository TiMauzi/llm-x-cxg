{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk import download\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import MBart50Tokenizer, MBartForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Konstruktikon:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b8ae702f33cd2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_sentences(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    construction_id = root.attrib.get('id')\n",
    "    try:\n",
    "        category, name = root.attrib.get('name').split(':')\n",
    "    except ValueError:\n",
    "        category = root.attrib.get('name')\n",
    "        name = None\n",
    "\n",
    "    sentences_data = []\n",
    "    \n",
    "    for sentence in root.findall('.//sentence'):\n",
    "        if sentence.attrib.get('uid') is None:\n",
    "            continue\n",
    "        uid = sentence.attrib.get('uid')\n",
    "        text = sentence.find('.//text').text.strip()\n",
    "        contextleft = sentence.find('.//contextleft').text.strip()\n",
    "        contextright = sentence.find('.//contextright').text.strip()\n",
    "        \n",
    "        sentences_data.append({\n",
    "            'uid': uid,\n",
    "            'text': text,\n",
    "            'contextleft': contextleft,\n",
    "            'contextright': contextright,\n",
    "            'construction_id': int(construction_id),\n",
    "            'category': category,\n",
    "            'name': name,\n",
    "        })\n",
    "\n",
    "    return sentences_data\n",
    "\n",
    "kee_list = []\n",
    "xml_directory = '../../data/constructicon/construction'\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_sentences(xml_file)\n",
    "        if data:\n",
    "            kee_list += data\n",
    "        time.sleep(.5)\n",
    "\n",
    "sentences = pd.DataFrame.from_dict(kee_list)\n",
    "sentences.set_index('uid', inplace=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90cdfcb72562113"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "constructions = sentences[['construction_id', 'category', 'name']].drop_duplicates()\n",
    "constructions.set_index('construction_id', inplace=True)\n",
    "constructions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8068e6b81ce76f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "constructions['category'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31634988893993d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_kee(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    construction_id = root.attrib.get('id')\n",
    "\n",
    "    ke_data = []\n",
    "    \n",
    "    for ke in root.findall('.//ke'):\n",
    "        ke_data.append({\n",
    "            'ke_id': int(ke.attrib.get(\"id\")),\n",
    "            'ke_name': ke.attrib.get(\"name\"),\n",
    "            'ke_coretype': ke.attrib.get(\"coretype\"),\n",
    "            'ke_attributes': ke.attrib.get(\"attributes\")\n",
    "        })\n",
    "\n",
    "    kee_data = []\n",
    "    \n",
    "    for kee in root.findall('.//kee'):\n",
    "        for ke in ke_data:\n",
    "            kee_data.append({\n",
    "                'kee_id': int(kee.attrib.get(\"id\")),\n",
    "                'kee_name': kee.attrib.get(\"name\"),\n",
    "                'construction_id': int(construction_id)\n",
    "            } | ke)\n",
    "\n",
    "    return kee_data\n",
    "\n",
    "kee_list = []\n",
    "xml_directory = '../../data/constructicon/construction'\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_kee(xml_file)\n",
    "        if data:\n",
    "            kee_list += data\n",
    "        time.sleep(.5)\n",
    "\n",
    "kees = pd.DataFrame.from_dict(kee_list)\n",
    "kees.set_index('kee_id', inplace=True)\n",
    "kees"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b39ac2bcf3e57d23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kees['ke_name'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4884c67fb347be9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kees['kee_name'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cdb48cc8c9d60a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die KEEs sind markiert wie folgt:\n",
    "\n",
    "```\n",
    "            <annotationset>\n",
    "            ...\n",
    "                <layer name=\"KEE-Negation:NEG_X_geschweige_denn_Y\" descriptor=\"KEE-Negation:NEG_X_geschweige_denn_Y\">\n",
    "                    <label start=\"72\" end=\"87\" name=\"geschweige_denn\" refid=\"GC_KEE:1\" itype=\"null\" groupid=\"0\"></label>\n",
    "                </layer>\n",
    "            ...\n",
    "            </annotationset>\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95c34b3628b665cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_masked_sentences(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sentences_data = []\n",
    "    \n",
    "    constr_id = int(root.attrib.get('id'))\n",
    "    \n",
    "    for sentence in root.findall('.//sentence'):\n",
    "        if sentence.attrib.get('uid') is None:\n",
    "            continue\n",
    "        \n",
    "        uid = sentence.attrib.get('uid')\n",
    "        text = sentence.find('.//text').text.strip()\n",
    "        \n",
    "        text_pos = []\n",
    "        kees = []\n",
    "        kees_idx = []\n",
    "        kes = []\n",
    "        kes_idx = []\n",
    "        \n",
    "        # Loop through annotations:\n",
    "        for layer in sentence.findall('.//layer'):\n",
    "            layer_name = layer.attrib.get('name')\n",
    "            \n",
    "            # Get the KEE annotations:\n",
    "            if \"KE-\" in layer_name or \"KEE-\" in layer_name:\n",
    "                # Loop over all KEEs or KEs:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    start = int(label.attrib.get('start'))\n",
    "                    end = int(label.attrib.get('end'))\n",
    "                    if \"KEE-\" in layer_name:\n",
    "                        kees.append(text[start:end])  # Read the KEE\n",
    "                        kees_idx.append((start, end))  # Log the position of the KEE\n",
    "                    else:\n",
    "                        kes.append(text[start:end])  # Read the KE\n",
    "                        kes_idx.append((start, end))  # Log the position of the KE\n",
    "            elif \"UPOS\" == layer_name:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    text_pos.append(label.attrib.get('name'))\n",
    "        \n",
    "        sentences_data.append({\n",
    "            'uid': uid,\n",
    "            'constr_id': constr_id,\n",
    "            'text': text,\n",
    "            'text_pos': text_pos,\n",
    "            'kees': kees,\n",
    "            'kees_idx': kees_idx,\n",
    "            'kes': kes,\n",
    "            'kes_idx': kes_idx,\n",
    "        })\n",
    "    return sentences_data\n",
    "    \n",
    "sentence_list = []\n",
    "xml_directory = ('../../data/constructicon/construction')\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_masked_sentences(xml_file)\n",
    "        if data:\n",
    "            sentence_list += data\n",
    "        time.sleep(.5)\n",
    "\n",
    "sentences = pd.DataFrame.from_dict(sentence_list)\n",
    "# sentences.set_index('uid', inplace=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2d0119da82288b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = sentences.explode([\"kees\", \"kees_idx\"], ignore_index=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "160806e6f09d44b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = sentences.explode([\"kes\", \"kes_idx\"], ignore_index=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "201814fb66b7f600"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_comapp = []\n",
    "csv_comapp = []\n",
    "errors = 0\n",
    "problematic_constructions = set()\n",
    "unproblematic_constructions = set()\n",
    "\n",
    "\n",
    "for _, row in tqdm(sentences.iterrows(), total=len(sentences)):\n",
    "    tokenized = str(row[\"text\"]).split()  # tokenizer.tokenize(...)\n",
    "    \n",
    "    try:\n",
    "        ke_start, ke_end = row[\"kes_idx\"]\n",
    "        tokenized_kees = str(row[\"kees\"]).split()  # tokenizer.tokenize(...)  # Split the KEEs if there are multi-word KEEs\n",
    "    except TypeError:\n",
    "        continue  # If there is nothing to mask or if there is no KEE, we can't use this example.\n",
    "        \n",
    "    masked_text = row[\"text\"][:ke_start] + \"<mask>\" + row[\"text\"][ke_end:]\n",
    "    tokenized_masked = str(masked_text).split()\n",
    "    # Rejoin \"<\", \"mask\" and \">\".\n",
    "    tokenized_masked = [\n",
    "        '<mask>' if tokenized_masked[i] == '<' else tokenized_masked[i]  # add \"<mask>\" back in instead of \"<\"\n",
    "        for i in range(len(tokenized_masked)) \n",
    "        if tokenized_masked[i] != 'mask' and tokenized_masked[i] != '>'  # remove \"mask\" and \">\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        kee_idx = [tokenized.index(tokenized_kee) for tokenized_kee in tokenized_kees]\n",
    "        kee_query_idx = []\n",
    "        for i in kee_idx:\n",
    "            if tokenized_masked.index(\"<mask>\") < i:  # indicates that the query_idx might have been shifted because the multi-token mask comes first\n",
    "                offset = len(tokenized) - len(tokenized_masked)\n",
    "                assert i - offset >= 0\n",
    "                kee_query_idx.append(i - offset)\n",
    "            else:\n",
    "                kee_query_idx.append(i)\n",
    "    except (ValueError, AssertionError) as e:\n",
    "        print(row[\"constr_id\"], type(e), e, \"... Continuing ...\")\n",
    "        errors += 1\n",
    "        problematic_constructions.add(row[\"constr_id\"])\n",
    "        continue\n",
    "    \n",
    "    unproblematic_constructions.add(row[\"constr_id\"])\n",
    "    \n",
    "    out_json = [{\n",
    "        \"label\": kee + str(row[\"constr_id\"]),\n",
    "        \"target1\": row[\"text\"], \n",
    "        \"target1_idx\": idx, \n",
    "        \"query\": masked_text,\n",
    "        \"query_idx\": q\n",
    "    } for kee, idx, q in zip(tokenized_kees, kee_idx, kee_query_idx)]  # Split the KEEs if there are multi-word KEEs\n",
    "    out_csv = [{\n",
    "        \"text\": row[\"text\"],\n",
    "        \"pos_tags\": row[\"text_pos\"],\n",
    "        \"mask\": row[\"text\"][ke_start:ke_end],\n",
    "        \"ambiguous_word\": kee,\n",
    "        \"label\": kee + str(row[\"constr_id\"])  # This will be the new token that we will add to the LLM (named \"<kee><i>\" where <kee> and <i> are replaced by the KEE's name and i will be replaced by the construction it appears in).\n",
    "    } for kee in tokenized_kees]\n",
    "    \n",
    "    json_comapp += out_json\n",
    "    csv_comapp += out_csv\n",
    "    \n",
    "with open(\"../../data/pseudowords/CoMaPP_all.json\", \"w\") as file:\n",
    "    json.dump(json_comapp, file, ensure_ascii=False)\n",
    "\n",
    "f\"{errors} elements from {len(problematic_constructions)} different constructions could not be saved as intended.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d52b8985dbc98a41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"../../data/pseudowords/CoMapp_Dataset.csv\", \"w+\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"label\", \"text\", \"pos_tags\", \"mask\", \"ambiguous_word\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_comapp)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab36bece68b956dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the pre-trained MBart-50 model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name, src_lang=\"de_DE\", tgt_lang=\"de_DE\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dda230e1ec31db17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a masked input sequence and convert it into tokens\n",
    "masked_sequence = \"Ich bin <mask> gegangen.\"\n",
    "input_ids = tokenizer.encode(masked_sequence, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "403ffd601033e1af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=100, num_return_sequences=15, num_beams=20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a78138e8e1934be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_texts = []\n",
    "for output in outputs:\n",
    "    predicted_texts.append(tokenizer.decode(output, skip_special_tokens=True))\n",
    "predicted_texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe6d677169786cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataframe which matches the index of the constructions dataframe and the constr_id of the kees dataframe\n",
    "kee_categories = sentences.join(constructions, on=\"constr_id\")[[\"constr_id\", \"kees\", \"category\", \"name\"]].drop_duplicates().reset_index(drop=True)\n",
    "kee_categories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38986199c8b8d87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the distinct names:\n",
    "kee_categories[\"name\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20a2198fa57892a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vergleiche wie ähnlich sich cased und uncased sind:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cfd14b8c2dcbbce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_text_1 = \"Wie Schulbücher den Mauerfall darstellen\"\n",
    "input_text_2 = \"wie Schulbücher den Mauerfall darstellen\"\n",
    "\n",
    "input_tokens_1 = tokenizer(input_text_1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_tokens_2 = tokenizer(input_text_2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "torch.stack((input_tokens_1.input_ids, input_tokens_2.input_ids))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31808de966d7d050"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output_1 = model(**input_tokens_1)\n",
    "    model_output_2 = model(**input_tokens_2)\n",
    "    \n",
    "hidden_states_1 = model_output_1.encoder_last_hidden_state\n",
    "hidden_states_2 = model_output_2.encoder_last_hidden_state\n",
    "\n",
    "sequence_emb_1 = hidden_states_1[:, 5, :]\n",
    "sequence_emb_2 = hidden_states_2[:, 5, :]\n",
    "\n",
    "sequence_emb_1, sequence_emb_2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15e3c29d9faf4b62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO Markiere die zu desambiguierenden Wörter\n",
    "# Position ergibt keinen Sinn, weil die maskierten Bereiche beliebig lang sein können (und damit der Index variabel ist in Abhängigkeit der länge der generierten Phrase.\n",
    "# Idee wäre festgelegtes Symbol (z. B. §) oder gar kein Symbol und einfach heuristisch von .index(wort) ausgehen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b4ea4a81f68480f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Text2TextGenerationPipeline, MBartForConditionalGeneration\n",
    "from transformers import MBart50Tokenizer\n",
    "\n",
    "model_name = \"facebook/mbart-large-50\"  # \"facebook/mbart-large-cc25\" \n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name, src_lang=\"de_DE\", tgt_lang=\"de_DE\")\n",
    "\n",
    "nlp = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "masked_sequence = \"Ich bin <mask> gegangen.\"\n",
    "\n",
    "predicted_texts = nlp(masked_sequence, max_length=100, num_return_sequences=5, num_beams=20)\n",
    "predicted_texts = [result['generated_text'] for result in predicted_texts]\n",
    "\n",
    "print(predicted_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "201dd41b6755ffc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = tokenizer(\"Moritz sitzt neben mir.\", text_target=\"Moritz sitzt neben mir.\")\n",
    "\n",
    "tokenizer.decode(x.input_ids)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c479ff576b140776"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer2 = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "x = tokenizer2(\"Moritz sitzt neben mir.\", text_target=\"Moritz sitzt neben mir.\")\n",
    "\n",
    "tokenizer2.decode(x.input_ids), tokenizer2.decode(x.labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c02a3826763f805b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tests mit LLMs (Llama-2-13B-German-Assistant-v4-GPTQ): Hierzu müssen die Definitionen der Konstruktionen geladen werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44fc339de61ce68f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_definitions(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    #definition = {\n",
    "    #    \"constr_id\": root.attrib.get('id'),\n",
    "    #    \"definition\": root.find('.//definition').text.strip()\n",
    "    #}\n",
    "    definition = root.find('.//definition').text.strip()\n",
    "    return definition\n",
    "\n",
    "xml_directory = '../../data/constructicon/construction'\n",
    "\n",
    "definitions = []\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_definitions(xml_file)\n",
    "        if data:\n",
    "            definitions.append(data)\n",
    "        time.sleep(.25)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cca496aa7d08337"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9705a95717e06c92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "definitions_clean = [re.sub(\"\\[/?\\w+::[\\w-]+\\]\", '\"', d) for d in definitions]\n",
    "definitions_clean = [re.sub(\"\\*\\*\", '\"', d) for d in definitions_clean]\n",
    "definitions_clean = [re.sub(\"\\_\\_+\", '', d) for d in definitions_clean]\n",
    "\n",
    "definitions_clean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8829e627a0423834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../out/definitions.pickle\", \"wb\") as file:\n",
    "    pickle.dump(definitions_clean, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0203ec01767cf1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
