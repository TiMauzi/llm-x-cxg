{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk import download\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import MBart50Tokenizer, MBartForConditionalGeneration\n",
    "\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data from the German FrameNet Constructicon:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b8ae702f33cd2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_sentences(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    construction_id = root.attrib.get('id')\n",
    "    try:\n",
    "        category, name = root.attrib.get('name').split(':')\n",
    "    except ValueError:\n",
    "        category = root.attrib.get('name')\n",
    "        name = None\n",
    "\n",
    "    sentences_data = []\n",
    "    \n",
    "    for sentence in root.findall('.//sentence'):\n",
    "        if sentence.attrib.get('uid') is None:\n",
    "            continue\n",
    "        uid = sentence.attrib.get('uid')\n",
    "        text = sentence.find('.//text').text.strip()\n",
    "        contextleft = sentence.find('.//contextleft').text.strip()\n",
    "        contextright = sentence.find('.//contextright').text.strip()\n",
    "        \n",
    "        sentences_data.append({\n",
    "            'uid': uid,\n",
    "            'text': text,\n",
    "            'contextleft': contextleft,\n",
    "            'contextright': contextright,\n",
    "            'construction_id': int(construction_id),\n",
    "            'category': category,\n",
    "            'name': name,\n",
    "        })\n",
    "\n",
    "    return sentences_data\n",
    "\n",
    "kee_list = []\n",
    "xml_directory = '../../data/constructicon/construction'\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_sentences(xml_file)\n",
    "        if data:\n",
    "            kee_list += data\n",
    "        time.sleep(.5)\n",
    "\n",
    "sentences = pd.DataFrame.from_dict(kee_list)\n",
    "sentences.set_index('uid', inplace=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90cdfcb72562113"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pickle contextleft and text with construction_id as index for generation purposes later:\n",
    "sentences[[\"construction_id\", \"contextleft\", \"text\"]].to_pickle(\"../../data/pseudowords/contextleft_text.pickle\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7801b841494cf79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "constructions = sentences[['construction_id', 'category', 'name']].drop_duplicates()\n",
    "constructions.set_index('construction_id', inplace=True)\n",
    "#constructions.to_csv(\"../../out/constructions.tsv\", sep=\"\\t\")\n",
    "constructions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8068e6b81ce76f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "constructions['category'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31634988893993d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_kee(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    construction_id = root.attrib.get('id')\n",
    "\n",
    "    ke_data = []\n",
    "    \n",
    "    for ke in root.findall('.//ke'):\n",
    "        ke_data.append({\n",
    "            'ke_id': int(ke.attrib.get(\"id\")),\n",
    "            'ke_name': ke.attrib.get(\"name\"),\n",
    "            'ke_coretype': ke.attrib.get(\"coretype\"),\n",
    "            'ke_attributes': ke.attrib.get(\"attributes\")\n",
    "        })\n",
    "\n",
    "    kee_data = []\n",
    "    \n",
    "    for kee in root.findall('.//kee'):\n",
    "        for ke in ke_data:\n",
    "            kee_data.append({\n",
    "                'kee_id': int(kee.attrib.get(\"id\")),\n",
    "                'kee_name': kee.attrib.get(\"name\"),\n",
    "                'construction_id': int(construction_id)\n",
    "            } | ke)\n",
    "\n",
    "    return kee_data\n",
    "\n",
    "kee_list = []\n",
    "xml_directory = '../../data/constructicon/construction'\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_kee(xml_file)\n",
    "        if data:\n",
    "            kee_list += data\n",
    "        time.sleep(.5)\n",
    "\n",
    "kees = pd.DataFrame.from_dict(kee_list)\n",
    "kees.set_index('kee_id', inplace=True)\n",
    "kees"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b39ac2bcf3e57d23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kees['ke_name'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4884c67fb347be9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kees['kee_name'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cdb48cc8c9d60a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The KEEs are marked as follows:\n",
    "\n",
    "```\n",
    "            <annotationset>\n",
    "            ...\n",
    "                <layer name=\"KEE-Negation:NEG_X_geschweige_denn_Y\" descriptor=\"KEE-Negation:NEG_X_geschweige_denn_Y\">\n",
    "                    <label start=\"72\" end=\"87\" name=\"geschweige_denn\" refid=\"GC_KEE:1\" itype=\"null\" groupid=\"0\"></label>\n",
    "                </layer>\n",
    "            ...\n",
    "            </annotationset>\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95c34b3628b665cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_masked_sentences(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sentences_data = []\n",
    "    \n",
    "    constr_id = int(root.attrib.get('id'))\n",
    "    \n",
    "    for sentence in root.findall('.//sentence'):\n",
    "        if sentence.attrib.get('uid') is None:\n",
    "            continue\n",
    "        \n",
    "        uid = sentence.attrib.get('uid')\n",
    "        text = sentence.find('.//text').text.strip()\n",
    "        \n",
    "        text_pos = []\n",
    "        text_xpos = []\n",
    "        text_dep = []\n",
    "        text_head = []\n",
    "        kees = []\n",
    "        kees_idx = []\n",
    "        kes = []\n",
    "        kes_idx = []\n",
    "        \n",
    "        # Loop through annotations:\n",
    "        for layer in sentence.findall('.//layer'):\n",
    "            layer_name = layer.attrib.get('name')\n",
    "            \n",
    "            # Get the KEE annotations:\n",
    "            if \"KE-\" in layer_name or \"KEE-\" in layer_name:\n",
    "                # Loop over all KEEs or KEs:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    start = int(label.attrib.get('start'))\n",
    "                    end = int(label.attrib.get('end'))\n",
    "                    if \"KEE-\" in layer_name:\n",
    "                        kees.append(text[start:end])  # Read the KEE\n",
    "                        kees_idx.append((start, end))  # Log the position of the KEE\n",
    "                    else:\n",
    "                        kes.append(text[start:end])  # Read the KE\n",
    "                        kes_idx.append((start, end))  # Log the position of the KE\n",
    "            elif \"UPOS\" == layer_name:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    text_pos.append(label.attrib.get('name'))\n",
    "            elif \"XPOS\" == layer_name:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    text_xpos.append(label.attrib.get('name'))\n",
    "            elif \"DEP_REL\" == layer_name:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    text_dep.append(label.attrib.get('name'))\n",
    "            elif \"DEP_HEAD\" == layer_name:\n",
    "                for label in layer.findall('../label'):\n",
    "                    text_head.append(label.attrib.get('name'))\n",
    "                    \n",
    "        sentences_data.append({\n",
    "            'uid': uid,\n",
    "            'constr_id': constr_id,\n",
    "            'text': text,\n",
    "            'text_pos': text_pos,\n",
    "            'text_xpos': text_xpos,\n",
    "            'text_dep': text_dep,\n",
    "            'text_head': text_head,\n",
    "            'kees': kees,\n",
    "            'kees_idx': kees_idx,\n",
    "            'kes': kes,\n",
    "            'kes_idx': kes_idx,\n",
    "        })\n",
    "    return sentences_data\n",
    "    \n",
    "sentence_list = []\n",
    "xml_directory = ('../../data/constructicon/construction')\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_masked_sentences(xml_file)\n",
    "        if data:\n",
    "            sentence_list += data\n",
    "        time.sleep(.5)\n",
    "\n",
    "sentences = pd.DataFrame.from_dict(sentence_list)\n",
    "# sentences.set_index('uid', inplace=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2d0119da82288b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = sentences.explode([\"kees\", \"kees_idx\"], ignore_index=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "160806e6f09d44b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = sentences.explode([\"kes\", \"kes_idx\"], ignore_index=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "201814fb66b7f600"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_comapp = []\n",
    "csv_comapp = []\n",
    "errors = 0\n",
    "problematic_constructions = set()\n",
    "unproblematic_constructions = set()\n",
    "\n",
    "\n",
    "for _, row in tqdm(sentences.iterrows(), total=len(sentences)):\n",
    "    tokenized = str(row[\"text\"]).split()\n",
    "    \n",
    "    try:\n",
    "        ke_start, ke_end = row[\"kes_idx\"]\n",
    "        tokenized_kees = str(row[\"kees\"]).split()  # Split the KEEs if there are multi-word KEEs\n",
    "    except TypeError:\n",
    "        continue  # If there is nothing to mask or if there is no KEE, we can't use this example.\n",
    "        \n",
    "    masked_text = row[\"text\"][:ke_start] + \"<mask>\" + row[\"text\"][ke_end:]\n",
    "    tokenized_masked = str(masked_text).split()\n",
    "    # Rejoin \"<\", \"mask\" and \">\".\n",
    "    tokenized_masked = [\n",
    "        '<mask>' if tokenized_masked[i] == '<' else tokenized_masked[i]  # add \"<mask>\" back in instead of \"<\"\n",
    "        for i in range(len(tokenized_masked)) \n",
    "        if tokenized_masked[i] != 'mask' and tokenized_masked[i] != '>'  # remove \"mask\" and \">\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        kee_idx = [tokenized.index(tokenized_kee) for tokenized_kee in tokenized_kees]\n",
    "        kee_query_idx = []\n",
    "        for i in kee_idx:\n",
    "            if tokenized_masked.index(\"<mask>\") < i:  # indicates that the query_idx might have been shifted because the multi-token mask comes first\n",
    "                offset = len(tokenized) - len(tokenized_masked)\n",
    "                assert i - offset >= 0\n",
    "                kee_query_idx.append(i - offset)\n",
    "            else:\n",
    "                kee_query_idx.append(i)\n",
    "    except (ValueError, AssertionError) as e:\n",
    "        print(row[\"constr_id\"], type(e), e, \"... Continuing ...\")\n",
    "        errors += 1\n",
    "        problematic_constructions.add(row[\"constr_id\"])\n",
    "        continue\n",
    "    \n",
    "    unproblematic_constructions.add(row[\"constr_id\"])\n",
    "    \n",
    "    out_json = [{\n",
    "        \"label\": kee + str(row[\"constr_id\"]),\n",
    "        \"target1\": row[\"text\"], \n",
    "        \"target1_idx\": idx, \n",
    "        \"query\": masked_text,\n",
    "        \"query_idx\": q\n",
    "    } for kee, idx, q in zip(tokenized_kees, kee_idx, kee_query_idx)]  # Split the KEEs if there are multi-word KEEs\n",
    "    out_csv = [{\n",
    "        \"text\": row[\"text\"],\n",
    "        \"pos_tags\": row[\"text_pos\"],\n",
    "        \"xpos_tags\": row[\"text_xpos\"],\n",
    "        \"dep_rels\": row[\"text_dep\"],\n",
    "        \"dep_heads\": row[\"text_head\"],\n",
    "        \"mask\": row[\"text\"][ke_start:ke_end],\n",
    "        \"ambiguous_word\": kee,\n",
    "        \"label\": kee + str(row[\"constr_id\"])  # This will be the new token that we will add to the LLM (named \"<kee><i>\" where <kee> and <i> are replaced by the KEE's name and i will be replaced by the construction it appears in).\n",
    "    } for kee in tokenized_kees]\n",
    "    \n",
    "    json_comapp += out_json\n",
    "    csv_comapp += out_csv\n",
    "    \n",
    "with open(\"../../data/pseudowords/CoMaPP_all.json\", \"w\") as file:\n",
    "    json.dump(json_comapp, file, ensure_ascii=False)\n",
    "\n",
    "f\"{errors} elements from {len(problematic_constructions)} different constructions could not be saved as intended.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d52b8985dbc98a41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"../../data/pseudowords/CoMapp_Dataset.csv\", \"w+\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"label\", \"text\", \"pos_tags\", \"xpos_tags\", \"dep_rels\", \"dep_heads\", \"mask\", \"ambiguous_word\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_comapp)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab36bece68b956dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the pre-trained MBart-50 model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name, src_lang=\"de_DE\", tgt_lang=\"de_DE\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dda230e1ec31db17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a masked input sequence and convert it into tokens\n",
    "masked_sequence = \"Ich bin <mask> gegangen.\"\n",
    "input_ids = tokenizer.encode(masked_sequence, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "403ffd601033e1af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=100, num_return_sequences=15, num_beams=20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a78138e8e1934be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_texts = []\n",
    "for output in outputs:\n",
    "    predicted_texts.append(tokenizer.decode(output, skip_special_tokens=True))\n",
    "predicted_texts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe6d677169786cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataframe which matches the index of the constructions dataframe and the constr_id of the kees dataframe\n",
    "kee_categories = sentences.join(constructions, on=\"constr_id\")[[\"constr_id\", \"kees\", \"category\", \"name\"]].drop_duplicates().reset_index(drop=True)\n",
    "kee_categories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38986199c8b8d87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the distinct names:\n",
    "kee_categories[\"name\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20a2198fa57892a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part is for preparing tests with Llama 2:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44fc339de61ce68f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_definitions(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    definition = root.find('.//definition').text.strip()\n",
    "    return definition\n",
    "\n",
    "xml_directory = '../../data/constructicon/construction'\n",
    "\n",
    "definitions = {}\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_definitions(xml_file)\n",
    "        if data:\n",
    "            definitions[int(constr_id)] = data\n",
    "        time.sleep(.25)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cca496aa7d08337"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9705a95717e06c92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "definitions_clean = {_: re.sub(\"\\[/?\\w+::[\\w-]+\\]\", '\"', d) for _, d in definitions.items()}\n",
    "definitions_clean = {_: re.sub(\"\\*\\*\", '\"', d) for _, d in definitions_clean.items()}\n",
    "definitions_clean = {_: re.sub(\"\\_\\_+\", '', d) for _, d in definitions_clean.items()}\n",
    "definitions_clean = {_: re.sub('\"\"+', '\"', d) for _, d in definitions_clean.items()}\n",
    "\n",
    "definitions_clean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8829e627a0423834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../out/definitions.pickle\", \"wb\") as file:\n",
    "    pickle.dump(definitions_clean, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0203ec01767cf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "given_examples = {k: set() for k in sentences[\"constr_id\"]}\n",
    "for sentence in sentences[[\"constr_id\", \"text\"]].values:\n",
    "    given_examples[sentence[0]].add(sentence[1])\n",
    "given_examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20cc28258a5f6a3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../../out/sentences.pickle\", \"wb\") as file:\n",
    "    pickle.dump(given_examples, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3c09068f37929d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do experiments in order to get `examples.pickle`..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46717c952bae385"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../../out/examples.pickle\", \"rb\") as file:\n",
    "    examples = pickle.load(file)\n",
    "    \n",
    "assert len(definitions_clean) == len(examples)\n",
    "list(zip(definitions_clean, examples))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1ea2518440df6fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
