{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook explores adding pseudoword embeddings as new embeddings to a BERT model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "839603333987fa39"
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:36.472930100Z",
     "start_time": "2023-10-06T16:31:36.431044100Z"
    }
   },
   "id": "1885ffc884adae38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the pseudoword embeddings:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a5f4b6ca3447959"
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.4791993 ,  0.6717983 ,  0.34754717, ..., -0.83745944,\n         3.1185284 ,  0.02498921],\n       [-1.1472282 ,  0.6415113 , -2.1778345 , ...,  1.7338743 ,\n        -1.3341838 ,  2.2364955 ],\n       [ 1.2592171 ,  1.4044372 ,  0.7233554 , ..., -0.6436405 ,\n         2.5080867 ,  0.63867843],\n       ...,\n       [ 0.7099659 ,  1.7526314 , -0.54122496, ...,  1.9021684 ,\n        -3.6823387 ,  0.61035985],\n       [ 0.5576828 , -1.8875732 ,  0.34652272, ...,  7.455317  ,\n         0.824571  ,  3.4515877 ],\n       [ 0.60624564,  2.9930663 , -0.77594584, ..., -1.3732387 ,\n        -2.6446538 ,  2.951031  ]], dtype=float32)"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudowords = np.load(\"../out/pseudowords-test.npy\")\n",
    "pseudowords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:36.474933300Z",
     "start_time": "2023-10-06T16:31:36.434053700Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the new token names:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3be05fa9ba7ff46a"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "['in1',\n 'in2',\n 'in3',\n 'in4',\n 'in5',\n 'in6',\n 'in7',\n 'in8',\n 'in9',\n 'in10',\n 'for1',\n 'for2',\n 'for3',\n 'for4',\n 'for5',\n 'for6',\n 'for7',\n 'for8',\n 'for9',\n 'for10',\n 'for11',\n 'for12',\n 'for13',\n 'for14',\n 'for15',\n 'started1',\n 'started2',\n 'started3',\n 'started4',\n 'started5',\n 'started6',\n 'started7',\n 'started8',\n 'started9',\n 'started10',\n 'had1',\n 'had2',\n 'had3',\n 'had4',\n 'had5',\n 'had6',\n 'had7',\n 'had8',\n 'had9',\n 'had10',\n 'had11',\n 'had12',\n 'had13',\n 'had14',\n 'had15',\n 'had16',\n 'had17',\n 'had18',\n 'had19',\n 'had20',\n 'had21',\n 'had22',\n 'had23',\n 'had24',\n 'had25',\n 'about1',\n 'about2',\n 'about3',\n 'about4',\n 'about5',\n 'about6',\n 'about7',\n 'about8',\n 'with1',\n 'with2',\n 'with3',\n 'with4',\n 'with5',\n 'with6',\n 'with7',\n 'with8',\n 'with9',\n 'with10',\n 'with11',\n 'on1',\n 'on2',\n 'on3',\n 'on4',\n 'on5',\n 'on6',\n 'on7',\n 'on8',\n 'on9',\n 'on10',\n 'run1',\n 'run2',\n 'run3',\n 'run4',\n 'run5',\n 'run6',\n 'run7',\n 'run8']"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../libs/pwibm/data/queries/single_target/MaPP_all.txt\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "new_tokens = [d[\"query\"].split()[d[\"query_idx\"]] for d in data]\n",
    "token_counts = {}\n",
    "bert_tokens = []\n",
    "\n",
    "for token in new_tokens:\n",
    "    # Check if the element is already in the dictionary\n",
    "    if token in token_counts:\n",
    "        # Increment the count for this element\n",
    "        token_counts[token] += 1\n",
    "    else:\n",
    "        # Initialize the count for this element to 1\n",
    "        token_counts[token] = 1\n",
    "\n",
    "    # Append the element with its count to the output list\n",
    "    bert_tokens.append(f\"{token}{token_counts[token]}\")\n",
    "\n",
    "bert_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:36.487795700Z",
     "start_time": "2023-10-06T16:31:36.441072700Z"
    }
   },
   "id": "cd2979f310bbe098"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the vanilla BERT model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a823993f6cd12d1"
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(28996, 768, padding_idx=0)"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-cased', return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model.bert.embeddings.word_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.108642900Z",
     "start_time": "2023-10-06T16:31:36.447925Z"
    }
   },
   "id": "36b0ec25d2a7b96d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add to existing embeddings:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c998c44f7fe5a64c"
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(29093, 768)"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_embeddings = torch.cat((model.bert.embeddings.word_embeddings.weight, torch.tensor(pseudowords)), dim=0)\n",
    "model.bert.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(combined_embeddings)\n",
    "model.bert.embeddings.word_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.135953600Z",
     "start_time": "2023-10-06T16:31:38.114107700Z"
    }
   },
   "id": "f75d926b56c56aa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add to existing tokens:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7660671b4583a798"
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 29093. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(29093, 768)"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(bert_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.181498900Z",
     "start_time": "2023-10-06T16:31:38.129812100Z"
    }
   },
   "id": "3e2a95a395a524aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try it with an example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d3af6077a3d297"
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]', 'I', 'started8', 'my', '[MASK]', '.', '[SEP]']"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(\"[CLS] I started8 my [MASK]. [SEP]\")\n",
    "masked_index = tokenized_text.index(\"[MASK]\")\n",
    "tokenized_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.183661Z",
     "start_time": "2023-10-06T16:31:38.139216600Z"
    }
   },
   "id": "c22ec3fb15b6e0bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert the tokens to indices:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b06b6272dae022f5"
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  101,   146, 29028,  1139,   103,   119,   102]])"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "input_ids = torch.tensor([input_ids])\n",
    "input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.213592300Z",
     "start_time": "2023-10-06T16:31:38.142377300Z"
    }
   },
   "id": "640da514d8412154"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the token:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db0a38b7d71e3727"
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "data": {
      "text/plain": "'for3'"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "predicted_token_id = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "predicted_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.216591900Z",
     "start_time": "2023-10-06T16:31:38.146893600Z"
    }
   },
   "id": "e04734b9262be065"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the top 100 tokens:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a71ff18c4d51f4c1"
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for3 tensor(174.5336)\n",
      "with11 tensor(147.5389)\n",
      "run3 tensor(143.0515)\n",
      "in8 tensor(133.6161)\n",
      "on10 tensor(115.6954)\n",
      "for14 tensor(111.0461)\n",
      "with5 tensor(110.1141)\n",
      "for11 tensor(106.6162)\n",
      "with9 tensor(106.5543)\n",
      "for4 tensor(101.0922)\n",
      "started10 tensor(94.2033)\n",
      "with4 tensor(91.6169)\n",
      "in7 tensor(84.1162)\n",
      "run4 tensor(82.7684)\n",
      "for2 tensor(80.3614)\n",
      "for1 tensor(78.2343)\n",
      "on5 tensor(77.7789)\n",
      "on6 tensor(77.5200)\n",
      "started7 tensor(75.1326)\n",
      "with10 tensor(74.9434)\n",
      "about7 tensor(71.8192)\n",
      "had18 tensor(71.6250)\n",
      "had14 tensor(71.1675)\n",
      "with6 tensor(69.6956)\n",
      "had8 tensor(69.6460)\n",
      "with7 tensor(69.3532)\n",
      "run2 tensor(69.2648)\n",
      "with8 tensor(63.9455)\n",
      "had12 tensor(63.8302)\n",
      "for15 tensor(58.1539)\n",
      "had20 tensor(53.7997)\n",
      "had17 tensor(48.5922)\n",
      "on8 tensor(47.8732)\n",
      "about8 tensor(44.9220)\n",
      "on7 tensor(43.3684)\n",
      "run8 tensor(42.9613)\n",
      "had11 tensor(42.6453)\n",
      "in10 tensor(42.6311)\n",
      "in9 tensor(41.5016)\n",
      "started6 tensor(33.6866)\n",
      "about5 tensor(33.0514)\n",
      "had6 tensor(29.8570)\n",
      "had15 tensor(27.9974)\n",
      "had10 tensor(24.6933)\n",
      "started8 tensor(23.1316)\n",
      "in2 tensor(23.0176)\n",
      "run6 tensor(19.5172)\n",
      "had16 tensor(19.2121)\n",
      "on9 tensor(10.9656)\n",
      "started5 tensor(10.4804)\n",
      "book tensor(8.8527)\n",
      "life tensor(8.3802)\n",
      "day tensor(8.0878)\n",
      "had23 tensor(8.0616)\n",
      "journal tensor(8.0475)\n",
      "story tensor(7.9985)\n",
      "business tensor(7.9858)\n",
      "own tensor(7.9457)\n",
      "career tensor(7.8219)\n",
      "magazine tensor(7.7654)\n",
      "account tensor(7.7175)\n",
      "blog tensor(7.5382)\n",
      "computer tensor(7.3763)\n",
      "laptop tensor(7.3587)\n",
      "work tensor(7.3118)\n",
      "paper tensor(7.3087)\n",
      "books tensor(7.1403)\n",
      "beer tensor(7.0847)\n",
      "homework tensor(7.0787)\n",
      "newspaper tensor(6.9355)\n",
      "job tensor(6.8971)\n",
      "novel tensor(6.8946)\n",
      "notebook tensor(6.7237)\n",
      "coffee tensor(6.6338)\n",
      "cigarette tensor(6.5997)\n",
      "shop tensor(6.5798)\n",
      "drink tensor(6.5643)\n",
      "hair tensor(6.4396)\n",
      "game tensor(6.3884)\n",
      "watch tensor(6.3169)\n",
      "thoughts tensor(6.3018)\n",
      "fire tensor(6.2390)\n",
      "shift tensor(6.2129)\n",
      "website tensor(6.1484)\n",
      "morning tensor(6.1238)\n",
      "store tensor(6.0765)\n",
      "diary tensor(6.0515)\n",
      "car tensor(6.0461)\n",
      "kitchen tensor(5.9777)\n",
      "list tensor(5.9662)\n",
      "studies tensor(5.9323)\n",
      "article tensor(5.9264)\n",
      "eyes tensor(5.8660)\n",
      "report tensor(5.8556)\n",
      "cell tensor(5.8095)\n",
      "letter tensor(5.7989)\n",
      "house tensor(5.7822)\n",
      "class tensor(5.7737)\n",
      "chapter tensor(5.7143)\n",
      "way tensor(5.6954)\n"
     ]
    }
   ],
   "source": [
    "top_k = 100\n",
    "predicted_token_ids = torch.topk(predictions[0, masked_index], top_k).indices\n",
    "predicted_token_probs = torch.topk(predictions[0, masked_index], top_k).values\n",
    "\n",
    "# Convert the predicted token IDs back to tokens\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
    "\n",
    "# Print the top 5 predictions and their probabilities\n",
    "for token, prob in zip(predicted_tokens, predicted_token_probs):\n",
    "    print(token, prob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.300395400Z",
     "start_time": "2023-10-06T16:31:38.212593400Z"
    }
   },
   "id": "412b1529163d9372"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the most probable word that is not part of the new embeddings:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6f8c075142cc733"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n"
     ]
    }
   ],
   "source": [
    "predicted_token_ids = torch.argmax(predictions[0, masked_index])\n",
    "vocab_size = len(tokenizer)\n",
    "# Find the highest predicted token with an ID lower than 28997\n",
    "for i in range(vocab_size):\n",
    "    if predicted_token_ids <= 28996:\n",
    "        break\n",
    "    predicted_token_ids = torch.argsort(predictions[0, masked_index], descending=True)[i]\n",
    "\n",
    "# Convert the predicted token ID back to a token\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_ids])[0]\n",
    "\n",
    "print(predicted_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.475028Z",
     "start_time": "2023-10-06T16:31:38.234372300Z"
    }
   },
   "id": "aa8ec7e84884477f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the top 5 words that are not part of the new embeddings:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23bd77356979b3e"
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book 8.8527250289917\n",
      "life 8.380229949951172\n",
      "day 8.087793350219727\n",
      "journal 8.047537803649902\n",
      "story 7.998483657836914\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted token IDs and their probabilities\n",
    "predicted_token_probs = predictions[0, masked_index]\n",
    "vocab_size = len(tokenizer)\n",
    "# Create a list to store the top 5 predictions and their probabilities\n",
    "top_5_predictions = []\n",
    "\n",
    "# Find the top 5 predicted tokens with IDs lower than 28997\n",
    "for i in range(vocab_size):\n",
    "    if len(top_5_predictions) >= 5 or i >= vocab_size:\n",
    "        break\n",
    "    token_id = torch.argsort(predicted_token_probs, descending=True)[i].item()\n",
    "    if token_id <= 28996:\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "        top_5_predictions.append((predicted_token, predicted_token_probs[token_id].item()))\n",
    "\n",
    "# Print the top 5 predictions and their probabilities\n",
    "for token, prob in top_5_predictions:\n",
    "    print(token, prob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T16:31:38.705422Z",
     "start_time": "2023-10-06T16:31:38.473474Z"
    }
   },
   "id": "e18dc0c71658a1b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
