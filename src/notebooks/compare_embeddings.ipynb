{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-02T17:23:37.684703600Z",
     "start_time": "2024-01-02T17:23:26.150038400Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BertForMaskedLM, BertTokenizer\n\u001B[0;32m     10\u001B[0m tqdm\u001B[38;5;241m.\u001B[39mpandas()\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\transformers\\__init__.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     28\u001B[0m     OptionalDependencyNotAvailable,\n\u001B[0;32m     29\u001B[0m     _LazyModule,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m     logging,\n\u001B[0;32m     47\u001B[0m )\n\u001B[0;32m     50\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mget_logger(\u001B[38;5;18m__name__\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdependency_versions_table\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deps\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m require_version, require_version_core\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# define which module versions we always want to check at run time\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# order specific notes:\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# - tqdm must be checked before tokenizers\u001B[39;00m\n\u001B[0;32m     25\u001B[0m pkgs_to_check_at_runtime \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtqdm\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyyaml\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     38\u001B[0m ]\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\transformers\\utils\\__init__.py:18\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#!/usr/bin/env python\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# coding=utf-8\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_full_repo_name  \u001B[38;5;66;03m# for backward compatibility\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1229\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\huggingface_hub-0.18.0-py3.8.egg\\huggingface_hub\\__init__.py:344\u001B[0m, in \u001B[0;36m_attach.<locals>.__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m attr_to_modules:\n\u001B[0;32m    343\u001B[0m     submod_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpackage_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_to_modules[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 344\u001B[0m     submod \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(submod_path)\n\u001B[0;32m    345\u001B[0m     attr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(submod, name)\n\u001B[0;32m    347\u001B[0m     \u001B[38;5;66;03m# If the attribute lives in a file (module) with the same\u001B[39;00m\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001B[39;00m\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;66;03m# the module is accessible on the package.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\huggingface_hub-0.18.0-py3.8.egg\\huggingface_hub\\hf_api.py:47\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     30\u001B[0m     Any,\n\u001B[0;32m     31\u001B[0m     BinaryIO,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m     overload,\n\u001B[0;32m     44\u001B[0m )\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m quote\n\u001B[1;32m---> 47\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPError\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m base_tqdm\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\requests\\__init__.py:147\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NullHandler\n\u001B[1;32m--> 147\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m packages, utils\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__version__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    149\u001B[0m     __author__,\n\u001B[0;32m    150\u001B[0m     __author_email__,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    158\u001B[0m     __version__,\n\u001B[0;32m    159\u001B[0m )\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m delete, get, head, options, patch, post, put, request\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\requests\\packages.py:16\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# This code exists for backwards compatibility reasons.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# I don't like it either. Just look the other way. :)\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m package \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murllib3\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124midna\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m---> 16\u001B[0m     \u001B[38;5;28mlocals\u001B[39m()[package] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28m__import__\u001B[39m(package)\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m# This traversal is apparently necessary such that the identities are\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# preserved (requests.packages.urllib3.* is urllib3.*)\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(sys\u001B[38;5;241m.\u001B[39mmodules):\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\idna\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackage_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      3\u001B[0m     IDNABidiError,\n\u001B[0;32m      4\u001B[0m     IDNAError,\n\u001B[0;32m      5\u001B[0m     InvalidCodepoint,\n\u001B[0;32m      6\u001B[0m     InvalidCodepointContext,\n\u001B[0;32m      7\u001B[0m     alabel,\n\u001B[0;32m      8\u001B[0m     check_bidi,\n\u001B[0;32m      9\u001B[0m     check_hyphen_ok,\n\u001B[0;32m     10\u001B[0m     check_initial_combiner,\n\u001B[0;32m     11\u001B[0m     check_label,\n\u001B[0;32m     12\u001B[0m     check_nfc,\n\u001B[0;32m     13\u001B[0m     decode,\n\u001B[0;32m     14\u001B[0m     encode,\n\u001B[0;32m     15\u001B[0m     ulabel,\n\u001B[0;32m     16\u001B[0m     uts46_remap,\n\u001B[0;32m     17\u001B[0m     valid_contextj,\n\u001B[0;32m     18\u001B[0m     valid_contexto,\n\u001B[0;32m     19\u001B[0m     valid_label_length,\n\u001B[0;32m     20\u001B[0m     valid_string_length,\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintranges\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m intranges_contain\n\u001B[0;32m     24\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIDNABidiError\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIDNAError\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalid_string_length\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     44\u001B[0m ]\n",
      "File \u001B[1;32m~\\.conda\\envs\\llm-cxg\\Lib\\site-packages\\idna\\core.py:6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Union, Optional\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintranges\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m intranges_contain\n\u001B[0;32m      8\u001B[0m _virama_combining_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m9\u001B[39m\n\u001B[0;32m      9\u001B[0m _alabel_prefix \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxn--\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1176\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1147\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:690\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:936\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1032\u001B[0m, in \u001B[0;36mget_code\u001B[1;34m(self, fullname)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1130\u001B[0m, in \u001B[0;36mget_data\u001B[1;34m(self, path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "matches = pd.read_csv(\"../../out/matches/matches_dep_constr_kelex.tsv\", sep=\"\\t\", header=0)\n",
    "matches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T17:23:37.687905900Z",
     "start_time": "2024-01-02T17:23:37.687905900Z"
    }
   },
   "id": "959bd8c58a2b3474",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../data/pseudowords/annotations.csv\", \"r\") as csv_file:\n",
    "    data = [row for row in csv.DictReader(csv_file)]\n",
    "    \n",
    "kelex_dict = {}\n",
    "for example in data:\n",
    "    kees = set()\n",
    "    for kee in eval(example[\"kees\"]):\n",
    "        kees |= set(kee.split())\n",
    "    kelex_dict[int(example[\"construction_id\"])] = kees\n",
    "\n",
    "kelex_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T17:23:37.690135300Z"
    }
   },
   "id": "e24dcbbb3879f23f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# cf. evaluate_comapp_bert.ipynb\n",
    "\n",
    "pseudowords = []\n",
    "for i in range(15):\n",
    "    pseudowords.append(np.load(f\"../../data/pseudowords/bert/pseudowords_comapp_bert_{i*37}_{i*37+37}.npy\"))\n",
    "pseudowords = np.concatenate(pseudowords)\n",
    "\n",
    "csv_data = []\n",
    "for i in range(1, 16):\n",
    "    csv_data.append(pd.read_csv(f\"../../data/pseudowords/bert/order_bert_{i}.csv\", sep=\";\", index_col=0, header=None, quotechar=\"|\", names=[\"order\", \"label\"]))\n",
    "csv_data = pd.concat(csv_data)\n",
    "\n",
    "bert_tokens = [d[0] for d in csv_data.values]\n",
    "bert_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T17:23:37.702799500Z",
     "start_time": "2024-01-02T17:23:37.693136300Z"
    }
   },
   "id": "851b7c703fd02695",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-german-cased', return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "combined_embeddings = torch.cat((model.bert.embeddings.word_embeddings.weight, torch.tensor(pseudowords)), dim=0)\n",
    "model.bert.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(combined_embeddings)\n",
    "model.bert.embeddings.word_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T17:23:37.697438900Z"
    }
   },
   "id": "419603064ae92a15",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(bert_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T17:23:37.699789Z"
    }
   },
   "id": "cf5cfc3ca11d6215",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After everything has been prepared, we can create a list of the contextual embeddings for the standard BERT tokens and the pseudoword tokens."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185ba1a6dbb81597"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "contextual_embeds = {}\n",
    "\n",
    "for constr, kelexes in tqdm(kelex_dict.items()):\n",
    "    contextual_embeds[constr] = {}\n",
    "    for kelex in kelexes:\n",
    "        if kelex + str(constr) in bert_tokens:\n",
    "            original_ids = tokenizer(kelex, return_tensors='pt')['input_ids']\n",
    "            pseudoword_ids = tokenizer(kelex + str(constr), return_tensors='pt')['input_ids']\n",
    "            with torch.no_grad():\n",
    "                original_outputs = model(original_ids, output_hidden_states=True)\n",
    "                pseudoword_outputs = model(pseudoword_ids, output_hidden_states=True)\n",
    "            original_contextual_embed = original_outputs.hidden_states[12][0][1:-1]\n",
    "            pseudoword_contextual_embed = pseudoword_outputs.hidden_states[12][0][1:-1]\n",
    "            contextual_embeds[constr][kelex] = (original_contextual_embed, pseudoword_contextual_embed)\n",
    "        else:\n",
    "            print(kelex + str(constr))\n",
    "\n",
    "contextual_embeds.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T17:23:37.701802600Z"
    }
   },
   "id": "710eba094aee385c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "contextual_embeds[10][\"geschweige\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T17:23:37.703800700Z"
    }
   },
   "id": "e667595f81687afe",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can compare the distance of the pseudowords to a sentence with the distance of the original token(s) to the same sentence. Since one pseudoword token can be equivalent to multiple original tokens, we need to take the average distance from all original tokens to compare this average distance to the distance of the pseudoword token."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f3dcb1fd1bb88c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def distances(row):\n",
    "    global contextual_embeds\n",
    "    constr = row[\"constr\"]\n",
    "    if constr not in contextual_embeds.keys():\n",
    "        print(\".\")\n",
    "        return pd.Series({'constr': row['constr'], 'kelex': None, 'sentence': row['sentence'], 'bert_sim': None, 'pseudword_sim': None, 'bert_euclidean': None, 'pseudword_euclidean': None, 'bert_manhattan': None, 'pseudword_manhattan': None})\n",
    "    \n",
    "    return_row = []\n",
    "    sentence = row[\"sentence\"]\n",
    "    sentence_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "    with torch.no_grad():\n",
    "        if sentence_ids.size(-1) > 512:\n",
    "            # sliding window approach\n",
    "            print(\"Sentence needs slicing, this may take a while ...\")\n",
    "            sentence_id_list = [sentence_ids[:, i:i + 512] for i in range(0, sentence_ids.size(-1)-512+1)]\n",
    "            outputs_list = [model(sentence_ids[:, i:i + 512], output_hidden_states=True) for i in range(0, sentence_ids.size(-1)-512+1)]\n",
    "        else:\n",
    "            sentence_id_list = [sentence_ids]\n",
    "            outputs_list = [model(sentence_ids, output_hidden_states=True)]\n",
    "    \n",
    "        for kelex, embeds in contextual_embeds[constr].items():\n",
    "            if kelex not in sentence:\n",
    "                continue\n",
    "            bert_sims = []\n",
    "            pseudoword_sims = []\n",
    "            bert_euclideans = []\n",
    "            pseudoword_euclideans = []\n",
    "            bert_manhattans = []\n",
    "            pseudoword_manhattans = []\n",
    "            for cur_sentence_ids, outputs in zip(sentence_id_list, outputs_list):\n",
    "                kelex_ids = [idx for idx, t in enumerate(cur_sentence_ids[0]) if t in tokenizer(kelex, return_tensors='pt')['input_ids'][0][1:-1]]\n",
    "                if len(kelex_ids) == 0:  # the KE-LEX is not in the current segment\n",
    "                    continue\n",
    "                sentence_contextual_embeds = outputs.hidden_states[12][0][kelex_ids]\n",
    "                \n",
    "                # Now let's compare BERT and pseudoword:\n",
    "                bert_sims.append(torch.mean(F.cosine_similarity(embeds[0], sentence_contextual_embeds, dim=-1)))\n",
    "                pseudoword_sims.append(torch.mean(F.cosine_similarity(embeds[1].expand_as(sentence_contextual_embeds), sentence_contextual_embeds, dim=-1)))\n",
    "                bert_euclideans.append(torch.mean(torch.norm(embeds[0]-sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                pseudoword_euclideans.append(torch.mean(torch.norm(embeds[1].expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                bert_manhattans.append(torch.mean(torch.norm(embeds[0]-sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                pseudoword_manhattans.append(torch.mean(torch.norm(embeds[1].expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                \n",
    "            bert_sim = torch.mean(torch.tensor(bert_sims))\n",
    "            pseudoword_sim = torch.mean(torch.tensor(pseudoword_sims))\n",
    "            bert_euclidean = torch.mean(torch.tensor(bert_euclideans))\n",
    "            pseudoword_euclidean = torch.mean(torch.tensor(pseudoword_euclideans))\n",
    "            bert_manhattan = torch.mean(torch.tensor(bert_manhattans))\n",
    "            pseudoword_manhattan = torch.mean(torch.tensor(pseudoword_manhattans))\n",
    "            return_row.append({'constr': row['constr'], 'kelex': kelex, 'sentence': row['sentence'], 'bert_sim': float(bert_sim), 'pseudword_sim': float(pseudoword_sim), 'bert_euclidean': float(bert_euclidean), 'pseudword_euclidean': float(pseudoword_euclidean), 'bert_manhattan': float(bert_manhattan), 'pseudword_manhattan': float(pseudoword_manhattan)})\n",
    "            \n",
    "        print(return_row)\n",
    "        return pd.Series(return_row)\n",
    "\n",
    "similarities = matches.progress_apply(distances, axis=1)\n",
    "similarities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T17:23:37.705799700Z"
    }
   },
   "id": "36f0de68c5e5b325",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "similarities.to_csv(f\"../../out/comapp/similarities_bert.tsv\", sep=\"\\t\", decimal=\",\")\n",
    "similarities.to_excel(f\"../../out/comapp/similarities_bert.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T17:23:37.707800100Z"
    }
   },
   "id": "5f5b96a5d081379c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
