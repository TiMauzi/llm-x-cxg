{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers==4.33.2\n",
    "!pip3 install optimum==1.13.2\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertForMaskedLM, BertTokenizer\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a91fc727a14b5829",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models\n",
    "First, let us load a LLAMA model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5f17982bdd552"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llama_name_or_path = 'TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ'\n",
    "llama = AutoModelForCausalLM.from_pretrained(llama_name_or_path,\n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"gptq-4bit-32g-actorder_True\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_name_or_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "456637d556b2408a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we also need a BERT model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c651dcacb134b05f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_base = BertForMaskedLM.from_pretrained('dbmdz/bert-base-german-cased', return_dict=True)\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "bert_base.bert.embeddings.word_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "726e0f53efcc0851",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data\n",
    "Now, we load some data we need. First, we need some definitions and their example sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c5fa08ecb9e0463"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../out/definitions.pickle\", \"rb\") as file:\n",
    "    definitions = pickle.load(file)\n",
    "with open(\"../../out/sentences.pickle\", \"rb\") as file:\n",
    "    sentences = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad92bdedb375375",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ff92e816e794049",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next up, we load the prepared BERT pseudoword embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3cee9e7567b3803"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pseudowords = []\n",
    "for i in range(15):\n",
    "    pseudowords.append(np.load(f\"../../data/pseudowords/bsbbert/pseudowords_comapp_bsbbert_{i*37}_{i*37+37}.npy\"))\n",
    "pseudowords = np.concatenate(pseudowords)\n",
    "pseudowords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d8c5efdd7eaee7a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "csv_data = []\n",
    "for i in range(1, 16):\n",
    "    csv_data.append(pd.read_csv(f\"../../data/pseudowords/bsbbert/order_bsbbert_{i}.csv\", sep=\";\", index_col=0, header=None, quotechar=\"|\", names=[\"order\", \"label\"]))\n",
    "csv_data = pd.concat(csv_data)\n",
    "csv_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69a15576307dd06",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, we define a lookup table to map from construction ids to the pseudowords more quickly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf39b7d49d760b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../data/pseudowords/annotations.csv\", \"r\") as csv_file:\n",
    "    data = [row for row in csv.DictReader(csv_file)]\n",
    "    \n",
    "kelex_dict = {}\n",
    "for example in data:\n",
    "    kees = set()\n",
    "    for kee in eval(example[\"kees\"]):\n",
    "        kees |= set(kee.split())\n",
    "    kelex_dict[int(example[\"construction_id\"])] = kees\n",
    "\n",
    "kelex_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b39cb1f93066ad",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These pseudowords are now added to BERT."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0b0b2e410df0403"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_tokens = [d[0] for d in csv_data.values]\n",
    "\n",
    "bert_tokens, len(bert_tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ef93397b16a6b6f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_embeddings = torch.cat((bert_base.bert.embeddings.word_embeddings.weight, torch.tensor(pseudowords)), dim=0)\n",
    "bert_base.bert.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(combined_embeddings)\n",
    "bert_base.bert.embeddings.word_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91547903c0f7f051",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_base_tokenizer.add_tokens(bert_tokens)\n",
    "bert_base.resize_token_embeddings(len(bert_base_tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c904c2bf3c4000a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we move both models to a GPU. If there is only one GPU, only the LLAMA model is moved there."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb866fc9bb5f3dd9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llama.to(\"cuda:0\")\n",
    "bert_device = \"cuda:1\" if torch.cuda.device_count() >= 2 else \"cpu\"\n",
    "bert_base.to(bert_device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c48fd32daf20b2ac",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generation\n",
    "Finally, we can start generating new examples. First we define a function which lets LLAMA propose a sentence. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4eddc2e4831a0f1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_examples(definition: str, sentence: str, temperature=0.75, top_p=0.95, top_k=1, max_new_tokens=1024):\n",
    "    if len(sentence) > 2:\n",
    "        prompt = lambda definition, sentence: f'''### User: Du bist kreativ und gewissenhaft. Hier ist eine Definition: {definition} Bilde neue Sätze gemäß dieser Definition. Gib die Sätze in einer Python-Liste aus. Gib sonst nichts aus.\n",
    "        ### Example: {sentence}\n",
    "        ### Assistant:[\"'''\n",
    "    else:\n",
    "        prompt = lambda definition, sentence: f'''### User: Du bist kreativ und gewissenhaft. Hier ist eine Definition: {definition} Bilde neue Sätze gemäß dieser Definition. Gib die Sätze in einer Python-Liste aus. Gib sonst nichts aus.\n",
    "        ### Assistant:[\"'''\n",
    "\n",
    "    prompt_length = len(prompt(definition, sentence))\n",
    "    output = []\n",
    "    first_output = \"\"\n",
    "    i = 100  # only try 100 times\n",
    "    while (\n",
    "        (not any([c.isalpha() for c in first_output]))\n",
    "        or any([x in first_output for x in {\"Konstruktion\", \"Satz\", \"Überschrift\", \"_\", \":\", \"XY\", \"XP\", \"X \", \"Y \", \"X.\", \"Y.\"}])\n",
    "        or re.search(r\".*\\].*\\[.*\", first_output)\n",
    "    ):\n",
    "        input_ids = llama_tokenizer(prompt(definition, sentence), return_tensors='pt').input_ids.cuda()\n",
    "        output = llama.generate(inputs=input_ids, temperature=temperature,\n",
    "                                do_sample=True, top_p=top_p, top_k=top_k,\n",
    "                                max_new_tokens=max_new_tokens)\n",
    "        output = llama_tokenizer.decode(output[0])[prompt_length:].strip()\n",
    "        output = re.findall('\\[.*\\]', output)\n",
    "        if len(output) > 0:\n",
    "            first_output = output[0]\n",
    "        i -= 1\n",
    "        print(i, end=\" \")\n",
    "        if i == 0:\n",
    "            first_output = \"[]\"\n",
    "            break\n",
    "        #print(f\"\\t{output}\")\n",
    "    #print(f\"\\n{output}\")\n",
    "    print()\n",
    "    return first_output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca7472db47f2cda",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To validate the output from LLAMA, we check the produced sentence for the existence of the necessary KE-lex. If it is not available, we try again. If it is available, we check whether the pseudoword or the general embedding is closer to the representation given in the sentence. We accept a sentence only if the pseudoword is closer.\n",
    "\n",
    "Load the contextual embeddings first (are prepared in `compare_embeddings.ipynb`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449ca6b008c956b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../out/comapp/contextual_embeds_ex.pickle\", \"rb\") as file:\n",
    "    contextual_embeds_ex = pickle.load(file)\n",
    "    \n",
    "contextual_embeds_ex[10].keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "248777bd1a83d53",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compare_distances(constr, sentence):\n",
    "    global contextual_embeds_ex\n",
    "    \n",
    "    sentence_ids = bert_base_tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "    # First, check whether the construction has ke-lex and if any ke-lex is in the sentence. Also, drop a sentence if it is way too long!\n",
    "    if (constr not in contextual_embeds_ex.keys()) or (not any([kelex in sentence for kelex in contextual_embeds_ex[constr].keys()])) or (sentence_ids.size(-1) > 512):\n",
    "        print(\".\", end=\"\")\n",
    "        return False\n",
    "    with torch.no_grad():\n",
    "        sentence_id_list = [sentence_ids]\n",
    "        outputs_list = [bert_base(sentence_ids.to(bert_device), output_hidden_states=True)]\n",
    "    \n",
    "        pseudoword_fitting = []\n",
    "        for kelex, embeds in contextual_embeds_ex[constr].items():\n",
    "            if kelex not in sentence:\n",
    "                continue\n",
    "            bert_sims = []\n",
    "            pseudoword_sims = []\n",
    "            bert_euclideans = []\n",
    "            pseudoword_euclideans = []\n",
    "            bert_manhattans = []\n",
    "            pseudoword_manhattans = []\n",
    "            for cur_sentence_ids, outputs in zip(sentence_id_list, outputs_list):\n",
    "                kelex_ids = [idx for idx, t in enumerate(cur_sentence_ids[0]) if t in bert_base_tokenizer(kelex, return_tensors='pt')['input_ids'][0][1:-1]]\n",
    "                if len(kelex_ids) == 0:  # the KE-LEX is not in the current segment\n",
    "                    continue\n",
    "                sentence_contextual_embeds = outputs.hidden_states[12][0][kelex_ids]\n",
    "                \n",
    "                # Now let's compare BERT and pseudoword:\n",
    "                bert_sims.append(torch.mean(F.cosine_similarity(embeds[0].to(bert_device), sentence_contextual_embeds, dim=-1)))\n",
    "                pseudoword_sims.append(torch.mean(F.cosine_similarity(embeds[1].to(bert_device).expand_as(sentence_contextual_embeds), sentence_contextual_embeds, dim=-1)))\n",
    "                bert_euclideans.append(torch.mean(torch.norm(embeds[0].to(bert_device)-sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                pseudoword_euclideans.append(torch.mean(torch.norm(embeds[1].to(bert_device).expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                bert_manhattans.append(torch.mean(torch.norm(embeds[0].to(bert_device)-sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                pseudoword_manhattans.append(torch.mean(torch.norm(embeds[1].to(bert_device).expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                \n",
    "            bert_sim = torch.mean(torch.tensor(bert_sims))\n",
    "            pseudoword_sim = torch.mean(torch.tensor(pseudoword_sims))\n",
    "            bert_euclidean = torch.mean(torch.tensor(bert_euclideans))\n",
    "            pseudoword_euclidean = torch.mean(torch.tensor(pseudoword_euclideans))\n",
    "            bert_manhattan = torch.mean(torch.tensor(bert_manhattans))\n",
    "            pseudoword_manhattan = torch.mean(torch.tensor(pseudoword_manhattans))\n",
    "            \n",
    "            pseudoword_fitting.append(any([pseudoword_sim >= bert_sim, pseudoword_euclidean <= bert_euclidean, pseudoword_manhattan <= bert_manhattan]))\n",
    "        return any(pseudoword_fitting)  # return True if for the pseudoword at least one metric is better than for any of the standard embeddings in the examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "610643563810b825",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_and_check_examples(constr, definition, sentence, temperature=0.75, max_new_tokens=1000, top_k=100, top_p=0.99, patience=10):\n",
    "    fitting_example = False\n",
    "    # Loop until the example is fitting the construction properly according to pseudowords:\n",
    "    example_list_fitting = []\n",
    "    while not fitting_example and patience:\n",
    "        patience -= 1\n",
    "        example = generate_examples(definition=definition, sentence=sentence, temperature=temperature, max_new_tokens=max_new_tokens, top_k=top_k, top_p=top_p)\n",
    "        example_list_fitting = []\n",
    "        try:\n",
    "            example_list = eval(example)\n",
    "            if example_list != []:\n",
    "                for e in example_list:     \n",
    "                    e_fit = compare_distances(constr, e)\n",
    "                    fitting_example = fitting_example or e_fit  # \"or\": at least one pseudoword needs to fit\n",
    "                    if e_fit:\n",
    "                        example_list_fitting.append(e)\n",
    "            print(fitting_example, example_list, example_list_fitting)\n",
    "        except:\n",
    "            pass\n",
    "    return example_list_fitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2257f1decd7cba9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for shot in [1, 0]:\n",
    "    examples = {}\n",
    "    if os.path.exists(f\"../../out/llama_bert/examples_{shot}_shot_plus_bert.pickle\"):\n",
    "        with open(f\"../../out/llama_bert/examples_{shot}_shot_plus_bert.pickle\", \"rb\") as file:\n",
    "            examples = pickle.load(file)\n",
    "\n",
    "    for k in tqdm(definitions.keys()):\n",
    "        if k in examples.keys():\n",
    "            continue  # have already generated examples for this construction\n",
    "\n",
    "        definition = definitions[k]\n",
    "        try:\n",
    "            sentence = str(list(sentences[int(k)])[0:shot])  # get some sentences\n",
    "        except KeyError:\n",
    "            print((\"[]\", \"[]\", \"This seems wrong...\"))\n",
    "            examples[k] = (\"[]\", \"[]\")\n",
    "            continue\n",
    "\n",
    "        example = generate_and_check_examples(\n",
    "            constr=k, definition=definition, sentence=sentence, temperature=0.75,\n",
    "            max_new_tokens=512, top_k=100, top_p=0.99\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            print((sentence, example))\n",
    "            examples[k] = (sentence, example)\n",
    "        except:\n",
    "            print((sentence, \"[]\"))\n",
    "            examples[k] = (sentence, \"[]\")\n",
    "        \n",
    "        print(\"=====\")\n",
    "\n",
    "        with open(f\"../../out/llama_bert/examples_{shot}_shot.pickle\", \"wb\") as file:\n",
    "            pickle.dump(examples, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322c70db2bca3a89",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
