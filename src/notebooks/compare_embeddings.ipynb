{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "matches = pd.read_csv(\"../../out/matches/matches_dep_constr.tsv\", sep=\"\\t\", header=0)\n",
    "matches[\"m\"] = matches.apply(lambda row: row[\"fuzziness (common dep)\"] * row[\"fuzziness (matches)\"], axis=1)\n",
    "matches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "959bd8c58a2b3474",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../data/pseudowords/annotations.csv\", \"r\") as csv_file:\n",
    "    data = [row for row in csv.DictReader(csv_file)]\n",
    "    \n",
    "kelex_dict = {}\n",
    "for example in data:\n",
    "    kees = set()\n",
    "    for kee in eval(example[\"kees\"]):\n",
    "        kees |= set(kee.split())\n",
    "    kelex_dict[int(example[\"construction_id\"])] = kees\n",
    "\n",
    "kelex_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24dcbbb3879f23f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# cf. bert_comapp_generate_examples.ipynb\n",
    "\n",
    "pseudowords = []\n",
    "for i in range(15):\n",
    "    pseudowords.append(np.load(f\"../../data/pseudowords/bsbbert/pseudowords_comapp_bsbbert_{i*37}_{i*37+37}.npy\"))\n",
    "pseudowords = np.concatenate(pseudowords)\n",
    "\n",
    "csv_data = []\n",
    "for i in range(1, 16):\n",
    "    csv_data.append(pd.read_csv(f\"../../data/pseudowords/bsbbert/order_bsbbert_{i}.csv\", sep=\";\", index_col=0, header=None, quotechar=\"|\", names=[\"order\", \"label\"]))\n",
    "csv_data = pd.concat(csv_data)\n",
    "\n",
    "bert_tokens = [d[0] for d in csv_data.values]\n",
    "bert_tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "851b7c703fd02695",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('dbmdz/bert-base-german-cased', return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "\n",
    "combined_embeddings = torch.cat((model.bert.embeddings.word_embeddings.weight, torch.tensor(pseudowords)), dim=0)\n",
    "model.bert.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(combined_embeddings)\n",
    "model.bert.embeddings.word_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "419603064ae92a15",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(bert_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf5cfc3ca11d6215",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b34c62fdbe0f005b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After everything has been prepared, we can create a list of the contextual embeddings for the standard BERT tokens and the pseudoword tokens."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185ba1a6dbb81597"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "contextual_embeds = {}\n",
    "\n",
    "for constr, kelexes in tqdm(kelex_dict.items()):\n",
    "    contextual_embeds[constr] = {}\n",
    "    for kelex in kelexes:\n",
    "        if kelex + str(constr) in bert_tokens:\n",
    "            original_ids = tokenizer(kelex, return_tensors='pt')['input_ids'].to(\"cuda:0\")\n",
    "            pseudoword_ids = tokenizer(kelex + str(constr), return_tensors='pt')['input_ids'].to(\"cuda:0\")\n",
    "            with torch.no_grad():\n",
    "                original_outputs = model(original_ids, output_hidden_states=True)\n",
    "                pseudoword_outputs = model(pseudoword_ids, output_hidden_states=True)\n",
    "            original_contextual_embed = original_outputs.hidden_states[12][0][1:-1]\n",
    "            pseudoword_contextual_embed = pseudoword_outputs.hidden_states[12][0][1:-1]\n",
    "            contextual_embeds[constr][kelex] = (original_contextual_embed, pseudoword_contextual_embed)\n",
    "        else:\n",
    "            print(kelex + str(constr))\n",
    "\n",
    "contextual_embeds.keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710eba094aee385c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "contextual_embeds[10][\"geschweige\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e667595f81687afe",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can compare the distance of the pseudowords to a sentence with the distance of the original token(s) to the same sentence. Since one pseudoword token can be equivalent to multiple original tokens, we need to take the average distance from all original tokens to compare this average distance to the distance of the pseudoword token."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f3dcb1fd1bb88c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def distances(row):\n",
    "    global contextual_embeds\n",
    "    global sentence_slices_cache\n",
    "    constr = row[\"constr\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "    \n",
    "    return_row = []\n",
    "    sentence_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "    # First, check whether the construction has ke-lex and if any ke-lex is in the sentence. Also, drop a sentence if it is way too long!\n",
    "    if (constr not in contextual_embeds.keys()) or (not any([kelex in sentence for kelex in contextual_embeds[constr].keys()])) or (sentence_ids.size(-1) > 512):\n",
    "        print(\".\", end=\"\")\n",
    "        return [{'constr': row['constr'], 'kelex': None, 'sentence': row['sentence'], \"fuzziness (common dep)\": row[\"fuzziness (common dep)\"], \"fuzziness (matches)\": row[\"fuzziness (common dep)\"], \"m\": row[\"m\"], 'bert_sim': None, 'pseudword_sim': None, 'bert_euclidean': None, 'pseudword_euclidean': None, 'bert_manhattan': None, 'pseudword_manhattan': None}]\n",
    "    with torch.no_grad():\n",
    "        sentence_id_list = [sentence_ids]\n",
    "        try:\n",
    "            outputs_list = [model(sentence_ids.to(\"cuda:0\"), output_hidden_states=True)]\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            model.to(\"cpu\")\n",
    "            outputs_list = [model(sentence_ids, output_hidden_states=True)]\n",
    "            model.to(\"cuda:0\")\n",
    "    \n",
    "        for kelex, embeds in contextual_embeds[constr].items():\n",
    "            if kelex not in sentence:\n",
    "                continue\n",
    "            bert_sims = []\n",
    "            pseudoword_sims = []\n",
    "            bert_euclideans = []\n",
    "            pseudoword_euclideans = []\n",
    "            bert_manhattans = []\n",
    "            pseudoword_manhattans = []\n",
    "            for cur_sentence_ids, outputs in zip(sentence_id_list, outputs_list):\n",
    "                kelex_ids = [idx for idx, t in enumerate(cur_sentence_ids[0]) if t in tokenizer(kelex, return_tensors='pt')['input_ids'][0][1:-1]]\n",
    "                if len(kelex_ids) == 0:  # the KE-LEX is not in the current segment\n",
    "                    continue\n",
    "                sentence_contextual_embeds = outputs.hidden_states[12][0][kelex_ids]\n",
    "                \n",
    "                # Now let's compare BERT and pseudoword:\n",
    "                bert_sims.append(torch.mean(F.cosine_similarity(embeds[0], sentence_contextual_embeds, dim=-1)))\n",
    "                pseudoword_sims.append(torch.mean(F.cosine_similarity(embeds[1].expand_as(sentence_contextual_embeds), sentence_contextual_embeds, dim=-1)))\n",
    "                bert_euclideans.append(torch.mean(torch.norm(embeds[0]-sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                pseudoword_euclideans.append(torch.mean(torch.norm(embeds[1].expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                bert_manhattans.append(torch.mean(torch.norm(embeds[0]-sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                pseudoword_manhattans.append(torch.mean(torch.norm(embeds[1].expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                \n",
    "            bert_sim = torch.mean(torch.tensor(bert_sims))\n",
    "            pseudoword_sim = torch.mean(torch.tensor(pseudoword_sims))\n",
    "            bert_euclidean = torch.mean(torch.tensor(bert_euclideans))\n",
    "            pseudoword_euclidean = torch.mean(torch.tensor(pseudoword_euclideans))\n",
    "            bert_manhattan = torch.mean(torch.tensor(bert_manhattans))\n",
    "            pseudoword_manhattan = torch.mean(torch.tensor(pseudoword_manhattans))\n",
    "            return_row.append({'constr': row['constr'], 'kelex': kelex, 'sentence': row['sentence'], \"fuzziness (common dep)\": row[\"fuzziness (common dep)\"], \"fuzziness (matches)\": row[\"fuzziness (common dep)\"], \"m\": row[\"m\"], 'bert_sim': float(bert_sim), 'pseudword_sim': float(pseudoword_sim), 'bert_euclidean': float(bert_euclidean), 'pseudword_euclidean': float(pseudoword_euclidean), 'bert_manhattan': float(bert_manhattan), 'pseudword_manhattan': float(pseudoword_manhattan)})\n",
    "            \n",
    "            if any([pseudoword_sim >= bert_sim, pseudoword_euclidean <= bert_euclidean, pseudoword_manhattan <= bert_manhattan]):\n",
    "                print(\"\\n\" + str(return_row))\n",
    "            else:\n",
    "                print(\":\", end=\"\")\n",
    "        return return_row\n",
    "\n",
    "similarities = pd.DataFrame()\n",
    "save = 1000\n",
    "for match in tqdm(matches.to_dict(orient=\"index\").values(), total=len(matches)):\n",
    "    similarities = pd.concat((similarities, pd.DataFrame(distances(match))), ignore_index=True)\n",
    "    if save > 0:\n",
    "        save -= 1\n",
    "    else:\n",
    "        save = 1000\n",
    "        similarities.to_csv(f\"../../out/comapp/similarities_bsbbert.tsv\", sep=\"\\t\", decimal=\",\", index=False)\n",
    "similarities.to_csv(f\"../../out/comapp/similarities_bsbbert.tsv\", sep=\"\\t\", decimal=\",\", index=False)\n",
    "similarities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a595de810f89333",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "similarities_kelex_only = similarities.dropna(ignore_index=True)\n",
    "similarities_kelex_only.to_csv(f\"../../out/comapp/similarities_bsbbert_kelex_only.tsv\", sep=\"\\t\", index=False)\n",
    "similarities_kelex_only"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "736f5099fa65cea7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Variant using averaged contextual embeds for the constructicon samples:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4faa36348664926e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c999732a795b2c2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../data/pseudowords/CoMaPP_all_bert.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data = [{\"example\": d[\"target1\"], \"example_pseudoword\": (\" \".join(d[\"target1\"].split()[:d[\"target1_idx\"]]) + \" \" + d[\"label\"] + \" \" + \" \".join(d[\"target1\"].split()[d[\"target1_idx\"]+1:])).strip(), \"pseudoword\": d[\"label\"], \"kelex_idx\": d[\"target1_idx\"]} for d in data]\n",
    "df = pd.DataFrame.from_dict(data).drop_duplicates(ignore_index=True)\n",
    "\n",
    "# sort by construction number\n",
    "df['index'] = df['pseudoword'].str.extract('(\\d+)').astype(int)\n",
    "df.set_index('index', inplace=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'construction'}, inplace=True)\n",
    "\n",
    "# sort by constructions and their pseudowords/kelex\n",
    "result_df = df.groupby(['construction', 'pseudoword']).agg({'example': list, 'example_pseudoword': list, 'kelex_idx': list}).reset_index()\n",
    "\n",
    "result_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e6f377b632a71d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This dictionary is for easier access of each pseudoword's fitting examples:\n",
    "result_dict = {}\n",
    "for index, row in result_df.iterrows():\n",
    "    construction = row['construction']\n",
    "    pseudoword = row['pseudoword']\n",
    "    example_list = row['example']\n",
    "    example_pseudoword_list = row[\"example_pseudoword\"]\n",
    "    kelex_idxs = row[\"kelex_idx\"]\n",
    "\n",
    "    if construction not in result_dict:\n",
    "        result_dict[construction] = {}\n",
    "\n",
    "    result_dict[construction][pseudoword] = (example_list, example_pseudoword_list, kelex_idxs)\n",
    "    \n",
    "result_dict[5][\"Und5\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3767212dcb351989",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "contextual_embeds_ex = {}\n",
    "\n",
    "for constr, kelexes in tqdm(result_dict.items()):\n",
    "    contextual_embeds_ex[constr] = {}\n",
    "    for kelex, (exs, exs_pseudo, kelex_idx) in tqdm(kelexes.items(), disable=True):\n",
    "        if kelex in bert_tokens:\n",
    "            original_ids = [tokenizer(ex, return_tensors='pt')['input_ids'] for ex in exs]\n",
    "            pseudoword_ids = [tokenizer(ex, return_tensors='pt')['input_ids'] for ex in exs_pseudo]\n",
    "            with torch.no_grad():\n",
    "                original_outputs = [model(o.to(\"cuda:0\"), output_hidden_states=True) for o in original_ids]\n",
    "                pseudoword_outputs = [model(p.to(\"cuda:0\"), output_hidden_states=True) for p in pseudoword_ids]\n",
    "            # The contextual embedding is calculated by getting the mean of all contextual embeddings for each example.\n",
    "            original_contextual_embed_ex = torch.mean(torch.stack(\n",
    "                [o_out.hidden_states[12][0][i+1:i+2] for o_out, i in zip(original_outputs, kelex_idx)]\n",
    "            ), dim=0)\n",
    "            pseudoword_contextual_embed_ex = torch.mean(torch.stack(\n",
    "                [p_out.hidden_states[12][0][i+1:i+2] for p_out, i in zip(pseudoword_outputs, kelex_idx)]\n",
    "            ), dim=0)\n",
    "            contextual_embeds_ex[constr][kelex.replace(str(constr), \"\")] = (original_contextual_embed_ex, pseudoword_contextual_embed_ex)\n",
    "        else:\n",
    "            print(kelex)\n",
    "\n",
    "contextual_embeds_ex.keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b85c5178f8e0b820",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../out/comapp/contextual_embeds_ex.pickle\", \"wb\") as file:\n",
    "    pickle.dump(contextual_embeds_ex, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "991d7cc896dac84f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../out/comapp/contextual_embeds_ex.pickle\", \"rb\") as file:\n",
    "    contextual_embeds_ex = pickle.load(file)\n",
    "    \n",
    "contextual_embeds_ex"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfa49b5fa547e747",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def distances_ex(row):\n",
    "    global contextual_embeds_ex\n",
    "    global sentence_slices_cache\n",
    "    constr = row[\"constr\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "    \n",
    "    return_row = []\n",
    "    sentence_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "    # First, check whether the construction has ke-lex and if any ke-lex is in the sentence. Also, drop a sentence if it is way too long!\n",
    "    if (constr not in contextual_embeds_ex.keys()) or (not any([kelex in sentence for kelex in contextual_embeds_ex[constr].keys()])) or (sentence_ids.size(-1) > 512):\n",
    "        print(\".\", end=\"\")\n",
    "        return [{'constr': row['constr'], 'kelex': None, 'sentence': row['sentence'], \"fuzziness (common dep)\": row[\"fuzziness (common dep)\"], \"fuzziness (matches)\": row[\"fuzziness (common dep)\"], \"m\": row[\"m\"], 'bert_sim': None, 'pseudword_sim': None, 'bert_euclidean': None, 'pseudword_euclidean': None, 'bert_manhattan': None, 'pseudword_manhattan': None}]\n",
    "    with torch.no_grad():\n",
    "        sentence_id_list = [sentence_ids]\n",
    "        outputs_list = [model(sentence_ids.to(\"cuda:0\"), output_hidden_states=True)]\n",
    "    \n",
    "        for kelex, embeds in contextual_embeds_ex[constr].items():\n",
    "            if kelex not in sentence:\n",
    "                continue\n",
    "            bert_sims = []\n",
    "            pseudoword_sims = []\n",
    "            bert_euclideans = []\n",
    "            pseudoword_euclideans = []\n",
    "            bert_manhattans = []\n",
    "            pseudoword_manhattans = []\n",
    "            for cur_sentence_ids, outputs in zip(sentence_id_list, outputs_list):\n",
    "                kelex_ids = [idx for idx, t in enumerate(cur_sentence_ids[0]) if t in tokenizer(kelex, return_tensors='pt')['input_ids'][0][1:-1]]\n",
    "                if len(kelex_ids) == 0:  # the KE-LEX is not in the current segment\n",
    "                    continue\n",
    "                sentence_contextual_embeds = outputs.hidden_states[12][0][kelex_ids]\n",
    "                \n",
    "                # Now let's compare BERT and pseudoword:\n",
    "                bert_sims.append(torch.mean(F.cosine_similarity(embeds[0], sentence_contextual_embeds, dim=-1)))\n",
    "                pseudoword_sims.append(torch.mean(F.cosine_similarity(embeds[1].expand_as(sentence_contextual_embeds), sentence_contextual_embeds, dim=-1)))\n",
    "                bert_euclideans.append(torch.mean(torch.norm(embeds[0]-sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                pseudoword_euclideans.append(torch.mean(torch.norm(embeds[1].expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=2, dim=-1)))\n",
    "                bert_manhattans.append(torch.mean(torch.norm(embeds[0]-sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                pseudoword_manhattans.append(torch.mean(torch.norm(embeds[1].expand_as(sentence_contextual_embeds) - sentence_contextual_embeds, p=1, dim=-1)))\n",
    "                \n",
    "            bert_sim = torch.mean(torch.tensor(bert_sims))\n",
    "            pseudoword_sim = torch.mean(torch.tensor(pseudoword_sims))\n",
    "            bert_euclidean = torch.mean(torch.tensor(bert_euclideans))\n",
    "            pseudoword_euclidean = torch.mean(torch.tensor(pseudoword_euclideans))\n",
    "            bert_manhattan = torch.mean(torch.tensor(bert_manhattans))\n",
    "            pseudoword_manhattan = torch.mean(torch.tensor(pseudoword_manhattans))\n",
    "            return_row.append({'constr': row['constr'], 'kelex': kelex, 'sentence': row['sentence'], \"fuzziness (common dep)\": row[\"fuzziness (common dep)\"], \"fuzziness (matches)\": row[\"fuzziness (common dep)\"], \"m\": row[\"m\"], 'bert_sim': float(bert_sim), 'pseudword_sim': float(pseudoword_sim), 'bert_euclidean': float(bert_euclidean), 'pseudword_euclidean': float(pseudoword_euclidean), 'bert_manhattan': float(bert_manhattan), 'pseudword_manhattan': float(pseudoword_manhattan)})\n",
    "            \n",
    "            if any([pseudoword_sim >= bert_sim, pseudoword_euclidean <= bert_euclidean, pseudoword_manhattan <= bert_manhattan]):\n",
    "                print(\"\\n\" + str(return_row))\n",
    "            else:\n",
    "                print(\":\", end=\"\")\n",
    "        return return_row\n",
    "\n",
    "similarities_ex = pd.DataFrame()\n",
    "save = 1000\n",
    "for match in tqdm(matches.to_dict(orient=\"index\").values(), total=len(matches)):\n",
    "    similarities_ex = pd.concat((similarities_ex, pd.DataFrame(distances_ex(match))), ignore_index=True)\n",
    "    if save > 0:\n",
    "        save -= 1\n",
    "    else:\n",
    "        save = 1000\n",
    "        similarities_ex.to_csv(f\"../../out/comapp/similarities_ex_bsbbert.tsv\", sep=\"\\t\", decimal=\",\", index=False)\n",
    "similarities_ex.to_csv(f\"../../out/comapp/similarities_ex_bsbbert.tsv\", sep=\"\\t\", decimal=\",\", index=False)\n",
    "similarities_ex"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef3e53ea9954a2e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "similarities_ex_kelex_only = similarities_ex.dropna(ignore_index=True)\n",
    "similarities_ex_kelex_only.to_csv(f\"../../out/comapp/similarities_ex_bsbbert_kelex_only.tsv\", sep=\"\\t\", index=False)\n",
    "similarities_ex_kelex_only"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82ebefe8ad9f21fb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "similarities[\"\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ad0bafeb9da04b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
