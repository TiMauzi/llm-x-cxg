...
Final loss = 2.4476905
*************************************************************************
After training:
query: <mask> - nach dem grossen New Yorker Börsenkrach - stieg auch in Arbon die Zahl der Arbeitslosen stark an #TOKEN# 1932 beispielsweise waren 360 Personen ohne Arbeit ) .
['output: Дешірресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресрес, score: 0.9207070469856262', 'output: Дешірресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресреслес, score: 0.8925048112869263', 'output: Дешірресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресреслеслес, score: 0.8869805932044983', 'output: Дешірресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресреслесресресресресресресресрес, score: 0.8846200704574585', 'output: Дешірресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресресреслесресресресресресрес, score: 0.8846073150634766']
*************************************************************************
----------------------------------------
Random 3
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [09:29<00:00,  8.79it/s]
Train Loss:  49%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                        | 2.4475629329681396/5 [09:29<09:53, 232.53s/it]
Train Loss:  49%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                        | 2.4475629329681396/5 [09:29<09:53, 232.50s/it]

Final loss = 2.447563
*************************************************************************
After training:
query: <mask> - nach dem grossen New Yorker Börsenkrach - stieg auch in Arbon die Zahl der Arbeitslosen stark an #TOKEN# 1932 beispielsweise waren 360 Personen ohne Arbeit ) .
['output: ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.8904004693031311', 'output: ☝☝☝☝☝☝☝ ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.872944176197052', 'output: ☝☝☝☝☝☝☝☝ ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.8715300559997559', 'output: ☝☝☝☝☝☝☝☝☝ ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.870005190372467', 'output: ☝☝☝☝☝☝☝☝☝☝ ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.8685991168022156']
*************************************************************************
----------------------------------------
Random 4
Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [10:14<00:00,  8.14it/s]
Train Loss:  49%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                        | 2.4475157260894775/5 [10:14<10:40, 250.88s/it]
Train Loss:  49%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                        | 2.4475157260894775/5 [10:13<10:40, 250.85s/it]

Final loss = 2.4475157
*************************************************************************
After training:
query: <mask> - nach dem grossen New Yorker Börsenkrach - stieg auch in Arbon die Zahl der Arbeitslosen stark an #TOKEN# 1932 beispielsweise waren 360 Personen ohne Arbeit ) .
['output: ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.7091159820556641', 'output: ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.7061677575111389', 'output: ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.7030755877494812', 'output: ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.6995746493339539', 'output: ☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝☝, score: 0.6961339712142944']
*************************************************************************
================================================================================
Construction:   1%|██▍                                                                                                                                                                                                                                                                              | 5/562 [2:38:15<343:38:28, 2221.02s/it]target 1: aus diesem Zusammenhang , der sogleich noch näher zu begründen ist , versteht man einen Zug als notwendig , den man vielfach an der Romanliteratur ( oder genauer gesagt : an der Geschichte des deutschen Entwicklungsromans ) bemerkt , aber fast immer als Mangel empfunden hat : daß alle im Roman vorkommenden Personen wohl charakterisiert und bis in ihre äußere Erscheinung hinein anschaulich geschildert sind , mit der einzigen Ausnahme des Helden selbst , der merkwürdig unbestimmt zu bleiben pflegt . in Wirklichkeit ist dieser Zug keineswegs ein Mangel , sondern hängt vielmehr untrennbar mit dem Wesen des " Helden " einer Erzählung zusammen : alle übrigen Personen sieht der Dichter von außen ., 25
query: aus diesem Zusammenhang , der sogleich noch näher zu begründen ist , versteht man einen Zug als notwendig , den man vielfach an der Romanliteratur ( oder genauer gesagt : an der Geschichte des deutschen Entwicklungsromans <mask>, 25
output: ['Auszug aus der Geschichte des deutschen Entwicklungsromans', 'Aus der Geschichte des deutschen Entwicklungsromans', 'an der Geschichte des deutschen Entwicklungsromans', 'Auszüge aus der Geschichte des deutschen Entwicklungsromans', 'Die Geschichte des deutschen Entwicklungsromans']
('aus diesem Zusammenhang , der sogleich noch näher zu begründen ist , versteht man einen Zug als notwendig , den man vielfach an der Romanliteratur #TOKEN# oder genauer gesagt : an der Geschichte des deutschen Entwicklungsromans <mask>', 25)
target 1: Seit Jahren bereitet das Drogenproblem Sorge ( genauer : jener Teil davon , den eine nicht ganz willkürfreie Gesetzgebung und Moral kriminalisiert hat ) , nicht ganz so lange und nicht ganz so intensiv der brutalisierende Einfluß der Fernsehprogramme ; seit vielen Jahrhunderten entlädt das Chaos der Menschenseele sich in Kriegen und Verbrechen , in Massenhysterien und individuellen Neurosen , in Wahnsinn und Selbstmord ., 6
query: Seit Jahren bereitet <mask> Sorge ( genauer : jener Teil davon , den eine nicht ganz willkürfreie Gesetzgebung und Moral kriminalisiert hat ) , nicht ganz so lange und nicht ganz so intensiv der brutalisierende Einfluß der Fernsehprogramme ; seit vielen Jahrhunderten entlädt das Chaos der Menschenseele sich in Kriegen und Verbrechen , in Massenhysterien und individuellen Neurosen , in Wahnsinn und Selbstmord ., 5
output: ['Seit Jahren bereitet uns das Chaos der Menschenseele Sorge ( genauer : jener Teil davon , den eine nicht', 'Seit Jahren bereitet das Chaos der Menschenseele Sorge ( genauer : jener Teil davon , den eine nicht ganz', 'Seit Jahren bereitet uns ein Teil der Menschheit Sorge ( genauer : jener Teil davon , den eine nicht ganz will', 'Seit Jahren bereitet ein Teil der Menschheit Sorge ( genauer : jener Teil davon , den eine nicht ganz willkür', 'Seit Jahren bereitet das Chaos der Menschen Sorge ( genauer : jener Teil davon , den eine nicht ganz willkür']
('Seit Jahren bereitet <mask> Sorge #TOKEN# genauer : jener Teil davon , den eine nicht ganz willkürfreie Gesetzgebung und Moral kriminalisiert hat ) , nicht ganz so lange und nicht ganz so intensiv der brutalisierende Einfluß der Fernsehprogramme ; seit vielen Jahrhunderten entlädt das Chaos der Menschenseele sich in Kriegen und Verbrechen , in Massenhysterien und individuellen Neurosen , in Wahnsinn und Selbstmord .', 5)
target 1: Leute , die gerne Sport treiben ( Tennis , Golf , Segeln ) , wandern oder die kulturgeschichtlichen Zeugen der Gegend aufsuchen , haben in dieser Zeit ihren Plausch ., 6
query: Leute , die gerne <mask> treiben ( Tennis , Golf , Segeln ) , wandern oder die kulturgeschichtlichen Zeugen der Gegend aufsuchen , haben in dieser Zeit ihren Plausch ., 6
output: ['Leute , die gerne Sport treiben ( Tennis , Golf , Segeln ) , wandern oder die kulturge', 'Leute , die gerne draußen Sport treiben ( Tennis , Golf , Segeln ) , wandern oder die kultur', 'Leute , die gerne etwas Sport treiben ( Tennis , Golf , Segeln ) , wandern oder die kultur', 'Leute , die gerne sportlich treiben ( Tennis , Golf , Segeln ) , wandern oder die kultur', 'Leute , die gerne spazieren gehen , Sport treiben ( Tennis , Golf , Segeln ) , wan']
('Leute , die gerne <mask> treiben #TOKEN# Tennis , Golf , Segeln ) , wandern oder die kulturgeschichtlichen Zeugen der Gegend aufsuchen , haben in dieser Zeit ihren Plausch .', 6)
----------------------------------------
Random 0
Construction:   1%|██▍                                                                                                                                                                                                                                                                              | 5/562 [2:38:31<294:19:46, 1902.31s/it]
Traceback (most recent call last):
  File "/home/tim_sockel/Privat/llm-x-cxg/src/pseudowords/get_kee_pseudowords_avg.py", line 414, in <module>
    co.coercion(group)  # , devices)
  File "/home/tim_sockel/Privat/llm-x-cxg/src/pseudowords/get_kee_pseudowords_avg.py", line 159, in coercion
    model = self._train(model, vec_targets, queries, targets1)
  File "/home/tim_sockel/Privat/llm-x-cxg/src/pseudowords/get_kee_pseudowords_avg.py", line 217, in _train
    target_idxs = torch.tensor([range(*i) for i in target_idxs], device="cuda:1")
ValueError: expected sequence of length 1 at dim 1 (got 2)
