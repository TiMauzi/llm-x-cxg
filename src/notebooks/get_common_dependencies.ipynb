{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from io import open\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from conllu import parse_incr, TokenList\n",
    "#from spacy_conll import init_parser\n",
    "#from spacy_conll.parser import ConllParser\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ud_hdt_sentences = {}\n",
    "all_ud_hdt_sentences = []\n",
    "faulty_sentences = {}\n",
    "filepath = \"../../libs/UD_German-HDT\"\n",
    "#nlp_conll = ConllParser(init_parser(\"de_core_news_sm\", \"spacy\"))\n",
    "for filename in tqdm(list(os.listdir(\"../../libs/UD_German-HDT\"))):\n",
    "    cur_sentences = []\n",
    "    if filename.endswith('.conllu'):\n",
    "        #for token_list in nlp_conll.parse_conll_file_as_spacy(os.path.join(filepath, filename)):\n",
    "        #    cur_sentences.append(token_list)\n",
    "        #ud_hdt_sentences[filename] = cur_sentences\n",
    "        #all_ud_hdt_sentences += cur_sentences\n",
    "            \n",
    "        data_file = open(os.path.join(filepath, filename), \"r\", encoding=\"utf-8\")\n",
    "        for token_list in parse_incr(data_file):\n",
    "            cur_sentences.append(token_list)\n",
    "        ud_hdt_sentences[filename] = cur_sentences\n",
    "        all_ud_hdt_sentences += cur_sentences\n",
    "        \n",
    "ud_hdt_sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c1c8f91864f1a3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "with open(\"../../data/pseudowords/annotations.csv\", \"r\") as csv_file:\n",
    "    data = [row for row in csv.DictReader(csv_file)]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96adaab4a9a1e59c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# In case some attributes are not pre-annotated, add them using spaCy:\n",
    "completed_data = []\n",
    "for incomplete_example in tqdm(data):\n",
    "    example = incomplete_example\n",
    "    nlp_example = nlp(example[\"text\"])\n",
    "    example[\"construction_id\"] = int(example[\"construction_id\"])\n",
    "    example[\"pos_tags\"] = []  # eval(example[\"pos_tags\"])\n",
    "    example[\"xpos_tags\"] = []  # eval(example[\"xpos_tags\"])\n",
    "    example[\"dep_rels\"] = []  # eval(example[\"dep_rels\"])\n",
    "    example[\"dep_heads\"] = []  # eval(example[\"dep_heads\"])\n",
    "    example[\"kees\"] = eval(example[\"kees\"])\n",
    "    if len(example[\"pos_tags\"]) == 0:\n",
    "        example[\"pos_tags\"] = [str(token.pos_) for token in nlp_example]\n",
    "    if len(example[\"xpos_tags\"]) == 0:\n",
    "        example[\"xpos_tags\"] = [str(token.tag_) for token in nlp_example]\n",
    "    if len(example[\"dep_rels\"]) == 0:\n",
    "        example[\"dep_rels\"] = [str(token.dep_).upper() for token in nlp_example]\n",
    "    if len(example[\"dep_heads\"]) == 0:\n",
    "        example[\"dep_heads\"] = [str(token.head).lower() for token in nlp_example]\n",
    "    completed_data.append(example)\n",
    "data = completed_data\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a287869f3cbf999c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group the dataset into a list of lists where the label of the dictionaries is identical:\n",
    "data.sort(key=lambda example: int(example[\"construction_id\"]))  # Grouping doesn't work without sorting first!\n",
    "data = {constr: list(group) for constr, group in itertools.groupby(data, key=lambda example: example[\"construction_id\"])}\n",
    "\n",
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b0d340c95cf7c10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(data.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "188064b82cd95632"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71909ccb7293a70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now try to exclude one sentence after the other to see if we can fill the empty sets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d761dd6bc7a47a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_common_deps(constr, group):\n",
    "    example_deps = []\n",
    "    for example in group:\n",
    "        # Variant with forms:\n",
    "        tokens = [token for token in nlp(example[\"text\"])]\n",
    "        head_poss = [token.head.pos_ for token in nlp(example[\"text\"])]\n",
    "        head_tags = [token.head.tag_ for token in nlp(example[\"text\"])]\n",
    "        example_deps.append((example, tokens, head_poss, head_tags))\n",
    "    \n",
    "    num_excluded = len(group) - 2  # start with only two sentences which are left in the group\n",
    "    common_deps = (set(), -1.0)\n",
    "    \n",
    "    while common_deps[1] != 0.0 and num_excluded > 0:\n",
    "        last_common_deps = common_deps\n",
    "        num_excluded -= 1\n",
    "        exclude = itertools.combinations(range(len(group)), num_excluded)  # try to take away different sentences from the group\n",
    "        \n",
    "        # exclude sentences from the current group\n",
    "        tries_left = 1000  # don't try for too long\n",
    "        for exclude_list in exclude:\n",
    "            if tries_left == 0:\n",
    "                break\n",
    "            else:\n",
    "                tries_left -= 1\n",
    "            smaller_deps = example_deps.copy()\n",
    "            if num_excluded > 0:\n",
    "                for e in sorted(list(exclude_list), reverse=True):  # sorting like this makes sure there are no index errors\n",
    "                    try: \n",
    "                        smaller_deps.pop(e)\n",
    "                    except:\n",
    "                        print(exclude_list)\n",
    "            group_deps = []\n",
    "            for example, tokens, head_poss, head_tags in smaller_deps:\n",
    "                group_deps.append([\n",
    "                    (str(token).lower(), str(dep), str(head))       # token -dep-> token\n",
    "                    for token, dep, head in zip(tokens, example[\"dep_rels\"], example[\"dep_heads\"])\n",
    "                ] + [\n",
    "                    (str(tag), str(dep), str(head))         # tag -dep-> token\n",
    "                    for tag, dep, head in zip(example[\"xpos_tags\"], example[\"dep_rels\"], example[\"dep_heads\"])\n",
    "                ] + [\n",
    "                    (str(pos), str(dep), str(head))         # pos -dep-> token\n",
    "                    for pos, dep, head in zip(example[\"pos_tags\"], example[\"dep_rels\"], example[\"dep_heads\"])\n",
    "                ] + [\n",
    "                    (str(token).lower(), str(dep), str(head_tag))   # token -dep-> tag\n",
    "                    for token, dep, head_tag in zip(tokens, example[\"dep_rels\"], head_tags)\n",
    "                ] + [\n",
    "                    (str(tag), str(dep), str(head_tag))     # tag -dep-> tag\n",
    "                    for tag, dep, head_tag in zip(example[\"xpos_tags\"], example[\"dep_rels\"], head_tags)\n",
    "                ] + [\n",
    "                    (str(pos), str(dep), str(head_tag))     # pos -dep-> tag\n",
    "                    for pos, dep, head_tag in zip(example[\"pos_tags\"], example[\"dep_rels\"], head_tags)\n",
    "                ] + [\n",
    "                    (str(token).lower(), str(dep), str(head_pos))   # token -dep-> pos\n",
    "                    for token, dep, head_pos in zip(tokens, example[\"dep_rels\"], head_poss)\n",
    "                ] + [\n",
    "                    (str(tag), str(dep), str(head_pos))     # tag -dep-> pos\n",
    "                    for tag, dep, head_pos in zip(example[\"xpos_tags\"], example[\"dep_rels\"], head_poss)\n",
    "                ] + [\n",
    "                    (str(pos), str(dep), str(head_pos))     # pos -dep-> pos\n",
    "                    for pos, dep, head_pos in zip(example[\"pos_tags\"], example[\"dep_rels\"], head_poss)\n",
    "                ])\n",
    "                \n",
    "            common_deps = set.intersection(*map(set, group_deps))\n",
    "            common_deps = (common_deps, len(smaller_deps) / len(group)) if len(common_deps) > 0 else (common_deps, 0.0)\n",
    "        \n",
    "            if common_deps[1] < last_common_deps[1] :\n",
    "                common_deps = last_common_deps\n",
    "                break  # found the best common set\n",
    "                \n",
    "    with open(\"../../out/matches/matches_filters.tsv\", \"a+\") as file:\n",
    "        file.write(str(constr) + \"\\t\" + str(1.0 - common_deps[1]) + \"\\t\" + str(common_deps[0]) + \"\\t\" + str(smaller_deps) + \"\\n\")\n",
    "    \n",
    "    return constr, common_deps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76eb9a3546cbf325"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../../out/matches/matches_filters.tsv\", \"w\") as file:\n",
    "    file.write(\"constr\\tfuzziness\\tfilter\\tsentences used\\n\")\n",
    "\n",
    "common_deps = {}\n",
    "for constr, group in tqdm(data.items(), desc=\"Construction\"):\n",
    "    # try finding a common depencency\n",
    "    common_deps[constr] = get_common_deps(constr, group)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "431d93f1f1fc6cf6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "common_deps = pd.read_csv(\"../../out/matches/matches_filters.tsv\", sep=\"\\t\", header=0)\n",
    "common_deps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "635aee312fe396e6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first filter for the existence of KE-LEX:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0d6239d3ffdeb3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "filtered_sentences = {}\n",
    "for constr in tqdm(common_deps[\"constr\"]):\n",
    "    filtered_sentences[constr] = []\n",
    "    for i, sentence in tqdm(enumerate(all_ud_hdt_sentences), total=len(all_ud_hdt_sentences), leave=False):\n",
    "        assert constr in data\n",
    "        kees_lists = data[constr]\n",
    "        # any list of ke-lex needs to match; but from this list, every ke-lex needs to match\n",
    "        # sentence.split(), so subwords do not match!\n",
    "        if any(all(keyword in [token[\"form\"] for token in sentence] for keyword in entry['kees']) for entry in kees_lists):\n",
    "            filtered_sentences[constr].append(sentence)\n",
    "            \n",
    "filtered_sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c55a5c522bcfa1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../out/matches/filtered_sentences.pickle\", \"wb\") as file:\n",
    "    pickle.dump(filtered_sentences, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9488295a2ca36ce",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can apply the filters we created before:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f2b65fba948f03"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../out/matches/matches_dep_constr.tsv\", \"w\") as file:\n",
    "    file.write(\"constr\\tfuzziness (common dep)\\tfuzziness (matches)\\tsentence\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10a7a3a4aef4b043",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for constr, deps, fuzziness in tqdm(zip(common_deps[\"constr\"], common_deps[\"filter\"], common_deps[\"fuzziness\"]), total=len(common_deps)):\n",
    "    example_outputs = 0\n",
    "    deps = eval(deps)\n",
    "    if set() in deps:\n",
    "        continue\n",
    "    for i, filtered_sentence in tqdm(enumerate(filtered_sentences[constr]), total=len(filtered_sentences[constr]), leave=False): # in ud_hdt_sentences[\"de_hdt-ud-test.conllu\"]\n",
    "        example_outputs += 1\n",
    "        found = 0.0  # all group elements have to be found in the sentence, so let's check that in the end!\n",
    "        for ex_token, ex_dep, ex_head in deps:\n",
    "            for token in filtered_sentence:\n",
    "                if not token[\"head\"]:  # for some reason, there is no token[\"head\"] sometimes...\n",
    "                    continue\n",
    "                deptoken = str(token[\"form\"]).lower()\n",
    "                deptoken_xpos = str(token[\"xpos\"]).lower()\n",
    "                deptoken_upos = str(token[\"upos\"]).lower()\n",
    "                deprel = str(token[\"deprel\"]).upper()\n",
    "                dephead = str(filtered_sentence[token[\"head\"] - 1][\"form\"]).lower()\n",
    "                dephead_xpos = filtered_sentence[token[\"head\"] - 1][\"xpos\"]\n",
    "                dephead_upos = filtered_sentence[token[\"head\"] - 1][\"upos\"]\n",
    "                    \n",
    "                if (\n",
    "                    ex_token in {deptoken, deptoken_xpos, deptoken_upos} and\n",
    "                    ex_dep == deprel and\n",
    "                    ex_head in {dephead, dephead_xpos, dephead_upos}\n",
    "                ):\n",
    "                    found += 1.0\n",
    "        if found > 0.0:  # if any matches have been found\n",
    "            if example_outputs <= 10:  # only print out 10 examples, otherwise this would take to much disk space\n",
    "                with open(f\"../../out/matches/graphics/{constr}_{i}.svg\", \"w\") as out_file:\n",
    "                    nlp_corpus_sentence = nlp(filtered_sentence.metadata[\"text\"])\n",
    "                    svg = displacy.render(nlp_corpus_sentence, style=\"dep\", jupyter=False)\n",
    "                    out_file.write(svg)\n",
    "            # note the amount of fuzziness (how many examples have been used to create the common dependency set) and how many of them actually matched\n",
    "            \n",
    "            score = (1.0 - fuzziness, 1.0 - (found / float(len(deps))))\n",
    "            with open(\"../../out/matches/matches_dep_constr.tsv\", \"a+\") as file:\n",
    "                file.write(str(constr) + \"\\t\" + str(score[0]) + \"\\t\" + str(score[1]) + \"\\t\" + filtered_sentence.metadata[\"text\"] + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc7ec6ce2372a66a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filters = pd.read_csv(\"../../out/matches/matches_filters.tsv\", sep=\"\\t\")\n",
    "filters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d8b76f112d8af19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matches = pd.read_csv(\"../../out/matches/matches_dep_constr.tsv\", sep=\"\\t\")\n",
    "matches"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c7aacbd77da9cdc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
