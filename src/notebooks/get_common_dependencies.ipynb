{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:13:34.699572500Z",
     "start_time": "2023-12-09T17:13:30.358090800Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "from io import open\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from conllu import parse_incr, TokenList\n",
    "#from spacy_conll import init_parser\n",
    "#from spacy_conll.parser import ConllParser\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_id(input_string):\n",
    "    match = re.search(r'\\d+$', input_string)\n",
    "    return int(match.group()) if match else -1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:13:34.714695300Z",
     "start_time": "2023-12-09T17:13:34.702855800Z"
    }
   },
   "id": "38082482227c71de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "935afe4a545f41a6a573f03017de736d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ud_hdt_sentences = {}\n",
    "all_ud_hdt_sentences = []\n",
    "faulty_sentences = {}\n",
    "filepath = \"../../libs/UD_German-HDT\"\n",
    "#nlp_conll = ConllParser(init_parser(\"de_core_news_sm\", \"spacy\"))\n",
    "for filename in tqdm(list(os.listdir(\"../../libs/UD_German-HDT\"))):\n",
    "    cur_sentences = []\n",
    "    if filename.endswith('.conllu'):\n",
    "        #for token_list in nlp_conll.parse_conll_file_as_spacy(os.path.join(filepath, filename)):\n",
    "        #    cur_sentences.append(token_list)\n",
    "        #ud_hdt_sentences[filename] = cur_sentences\n",
    "        #all_ud_hdt_sentences += cur_sentences\n",
    "            \n",
    "        data_file = open(os.path.join(filepath, filename), \"r\", encoding=\"utf-8\")\n",
    "        for token_list in parse_incr(data_file):\n",
    "            cur_sentences.append(token_list)\n",
    "        ud_hdt_sentences[filename] = cur_sentences\n",
    "        all_ud_hdt_sentences += cur_sentences\n",
    "        \n",
    "ud_hdt_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-09T17:13:34.709578600Z"
    }
   },
   "id": "9c1c8f91864f1a3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "with open(\"../../data/pseudowords/CoMaPP_Dataset.csv\", \"r\") as csv_file:\n",
    "    data = [row for row in csv.DictReader(csv_file)]\n",
    "\n",
    "# In case some attributes are not pre-annotated, add them using spaCy:\n",
    "completed_data = []\n",
    "for incomplete_example in tqdm(data):\n",
    "    example = incomplete_example\n",
    "    nlp_example = nlp(example[\"text\"])\n",
    "    example[\"pos_tags\"] = eval(example[\"pos_tags\"])\n",
    "    example[\"xpos_tags\"] = eval(example[\"xpos_tags\"])\n",
    "    example[\"dep_rels\"] = eval(example[\"dep_rels\"])\n",
    "    example[\"dep_heads\"] = eval(example[\"dep_heads\"])\n",
    "    if len(example[\"pos_tags\"]) == 0:\n",
    "        example[\"pos_tags\"] = [str(token.pos_) for token in nlp_example]\n",
    "    if len(example[\"xpos_tags\"]) == 0:\n",
    "        example[\"xpos_tags\"] = [str(token.tag_) for token in nlp_example]\n",
    "    if len(example[\"dep_rels\"]) == 0:\n",
    "        example[\"dep_rels\"] = [str(token.dep_).upper() for token in nlp_example]\n",
    "    # if len(example[\"dep_heads\"]) == 0:\n",
    "    example[\"dep_heads\"] = [str(token.head).lower() for token in nlp_example]\n",
    "    completed_data.append(example)\n",
    "data = completed_data\n",
    "        \n",
    "\n",
    "# Group the dataset into a list of lists where the label of the dictionaries is identical:\n",
    "data.sort(key=lambda x: get_id(x[\"label\"]))  # Grouping doesn't work without sorting first!\n",
    "data = {constr: list(group) for constr, group in itertools.groupby(data, key=lambda example: get_id(example[\"label\"]))}\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4b0d340c95cf7c10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(data.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "188064b82cd95632"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "common_deps = {}\n",
    "for constr, group in tqdm(data.items()):\n",
    "    group_deps = []\n",
    "    for example in group:\n",
    "        # Variant with forms:\n",
    "        tokens = [token for token in nlp(example[\"text\"])]\n",
    "        head_poss = [token.head.pos_ for token in nlp(example[\"text\"])]\n",
    "        head_tags = [token.head.tag_ for token in nlp(example[\"text\"])]\n",
    "        \n",
    "        group_deps.append([\n",
    "            (str(token).lower(), str(dep), str(head))       # token -dep-> token\n",
    "            for token, dep, head in zip(tokens, example[\"dep_rels\"], example[\"dep_heads\"])\n",
    "        ] + [\n",
    "            (str(tag), str(dep), str(head))         # tag -dep-> token\n",
    "            for tag, dep, head in zip(example[\"xpos_tags\"], example[\"dep_rels\"], example[\"dep_heads\"])\n",
    "        ] + [\n",
    "            (str(pos), str(dep), str(head))         # pos -dep-> token\n",
    "            for pos, dep, head in zip(example[\"pos_tags\"], example[\"dep_rels\"], example[\"dep_heads\"])\n",
    "        ] + [\n",
    "            (str(token).lower(), str(dep), str(head_tag))   # token -dep-> tag\n",
    "            for token, dep, head_tag in zip(tokens, example[\"dep_rels\"], head_tags)\n",
    "        ] + [\n",
    "            (str(tag), str(dep), str(head_tag))     # tag -dep-> tag\n",
    "            for tag, dep, head_tag in zip(example[\"xpos_tags\"], example[\"dep_rels\"], head_tags)\n",
    "        ] + [\n",
    "            (str(pos), str(dep), str(head_tag))     # pos -dep-> tag\n",
    "            for pos, dep, head_tag in zip(example[\"pos_tags\"], example[\"dep_rels\"], head_tags)\n",
    "        ] + [\n",
    "            (str(token).lower(), str(dep), str(head_pos))   # token -dep-> pos\n",
    "            for token, dep, head_pos in zip(tokens, example[\"dep_rels\"], head_poss)\n",
    "        ] + [\n",
    "            (str(tag), str(dep), str(head_pos))     # tag -dep-> pos\n",
    "            for tag, dep, head_pos in zip(example[\"xpos_tags\"], example[\"dep_rels\"], head_poss)\n",
    "        ] + [\n",
    "            (str(pos), str(dep), str(head_pos))     # pos -dep-> pos\n",
    "            for pos, dep, head_pos in zip(example[\"pos_tags\"], example[\"dep_rels\"], head_poss)\n",
    "        ])\n",
    "        \n",
    "    common_deps[constr] = set.intersection(*map(set, group_deps))  # get only the dependencies that are a match in all examples of one construction\n",
    "common_deps"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a6307e80ec126cc0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matches = {}\n",
    "scores = {}\n",
    "for constr, group in tqdm(common_deps.items()):\n",
    "    matches[constr] = {}\n",
    "    if set() in group:\n",
    "        continue\n",
    "    for i, corpus_sentence in enumerate(all_ud_hdt_sentences): # in ud_hdt_sentences[\"de_hdt-ud-test.conllu\"]\n",
    "        missing = 0.0  # all group elements have to be found in the sentence, so let's check that in the end!\n",
    "        for ex_token, ex_dep, ex_head in group:\n",
    "            for token in corpus_sentence:\n",
    "                if not token[\"head\"]:  # for some reason, there is no token[\"head\"] sometimes...\n",
    "                    continue\n",
    "                deptoken = str(token[\"form\"]).lower()\n",
    "                deptoken_xpos = str(token[\"xpos\"]).lower()\n",
    "                deptoken_upos = str(token[\"upos\"]).lower()\n",
    "                deprel = str(token[\"deprel\"]).upper()\n",
    "                dephead = str(corpus_sentence[token[\"head\"]-1][\"form\"]).lower()\n",
    "                dephead_xpos = corpus_sentence[token[\"head\"]-1][\"xpos\"]\n",
    "                dephead_upos = corpus_sentence[token[\"head\"]-1][\"upos\"]\n",
    "                    \n",
    "                if (\n",
    "                    ex_token in {deptoken, deptoken_xpos, deptoken_upos} and\n",
    "                    ex_dep == deprel and\n",
    "                    ex_head in {dephead, dephead_xpos, dephead_upos}\n",
    "                ):\n",
    "                    missing += 1.0\n",
    "            if missing > 0.0:  # if any matches have been found\n",
    "                with open(f\"../../out/matches/graphics/{constr}_{i}.svg\", \"w\") as out_file:\n",
    "                    nlp_corpus_sentence = nlp(corpus_sentence.metadata[\"text\"])\n",
    "                    svg = displacy.render(nlp_corpus_sentence, style=\"dep\", jupyter=False)\n",
    "                    out_file.write(svg)\n",
    "                matches[constr][corpus_sentence.metadata[\"text\"]] = missing / float(len(group))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "dc7ec6ce2372a66a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matches.values()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "36d95c0e64284218"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../../out/matches_dep_constr.txt\", \"w\") as file:\n",
    "    for key, match in matches.items(): \n",
    "        file.write(str(key) + \":\\n\")\n",
    "        for sentence, score in match.items():\n",
    "            file.write(\"\\t\" + str(score * 100) + \"%\\t\" + sentence + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cc1e0f02cd4865b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
