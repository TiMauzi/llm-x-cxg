{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers==4.33.2\n",
    "!pip3 install optimum==1.13.2\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertForMaskedLM, BertTokenizer\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a91fc727a14b5829"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models\n",
    "First, let us load a LLAMA model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5f17982bdd552"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llama_name_or_path = 'TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ'\n",
    "llama = AutoModelForCausalLM.from_pretrained(llama_name_or_path,\n",
    "                                             device_map='auto',\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"gptq-4bit-32g-actorder_True\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_name_or_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "456637d556b2408a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we also need a BERT model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c651dcacb134b05f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_base = BertForMaskedLM.from_pretrained('dbmdz/bert-base-german-cased', return_dict=True)\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "bert_base.bert.embeddings.word_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "726e0f53efcc0851"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data\n",
    "Now, we load some data we need. First, we need some "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c5fa08ecb9e0463"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad92bdedb375375"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../data/pseudowords/CoMaPP_all_bert.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data = [{\"example\": d[\"target1\"], \"query\": (\" \".join(d[\"query\"].split()[:d[\"query_idx\"]]) + \" \" + d[\"label\"] + \" \" + \" \".join(d[\"query\"].split()[d[\"query_idx\"]+1:])).strip(), \"pseudoword\": d[\"label\"]} for d in data]\n",
    "df = pd.DataFrame.from_dict(data).drop_duplicates(ignore_index=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "486bb05f7f078cd7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"definitions.pickle\", \"rb\") as file:\n",
    "    definitions = pickle.load(file)\n",
    "with open(\"sentences.pickle\", \"rb\") as file:\n",
    "    sentences = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd3cb71efee32218"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_examples(definition: str, sentence: str, temperature=0.75, top_p=0.95, top_k=1, max_new_tokens=1024):\n",
    "    if len(sentence) > 0:\n",
    "        prompt = lambda definition, sentence: f'''### User: Du bist kreativ und gewissenhaft. Hier ist eine Definition: {definition} Bilde neue Sätze gemäß dieser Definition. Gib die Sätze in einer Python-Liste aus. Gib sonst nichts aus.\n",
    "        ### Example: {sentence}\n",
    "        ### Assistant:[\"'''\n",
    "    else:\n",
    "        prompt = lambda definition, sentence: f'''### User: Du bist kreativ und gewissenhaft. Hier ist eine Definition: {definition} Bilde neue Sätze gemäß dieser Definition. Gib die Sätze in einer Python-Liste aus. Gib sonst nichts aus.\n",
    "        ### Assistant:[\"'''\n",
    "\n",
    "    prompt_length = len(prompt(definition, sentence))\n",
    "    output = []\n",
    "    first_output = \"\"\n",
    "    i = 100  # only try 100 times\n",
    "    while (\n",
    "        (not any([c.isalpha() for c in first_output]))\n",
    "        or any([x in first_output for x in {\"Konstruktion\", \"Satz\", \"Überschrift\", \"_\", \":\", \"XY\", \"XP\", \"X \", \"Y \", \"X.\", \"Y.\"}])\n",
    "        or re.search(r\".*\\].*\\[.*\", first_output)\n",
    "    ):\n",
    "        input_ids = llama_tokenizer(prompt(definition, sentence), return_tensors='pt').input_ids.cuda()\n",
    "        output = llama.generate(inputs=input_ids, temperature=temperature,\n",
    "                                do_sample=True, top_p=top_p, top_k=top_k,\n",
    "                                max_new_tokens=max_new_tokens)\n",
    "        output = llama_tokenizer.decode(output[0])[prompt_length:].strip()\n",
    "        output = re.findall('\\[.*\\]', output)\n",
    "        if len(output) > 0:\n",
    "            first_output = output[0]\n",
    "        i -= 1\n",
    "        print(i, end=\" \")\n",
    "        if i == 0:\n",
    "            first_output = \"[]\"\n",
    "            break\n",
    "        #print(f\"\\t{output}\")\n",
    "    #print(f\"\\n{output}\")\n",
    "    print()\n",
    "    return first_output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca7472db47f2cda"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_and_check_examples(definition, sentence, temperature=0.75, max_new_tokens=1000, top_k=100, top_p=0.99):\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2257f1decd7cba9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for shot in range(1, 2):\n",
    "    examples = {}\n",
    "    if os.path.exists(f\"examples_{shot}_shot_plus_bert.pickle\"):\n",
    "        with open(f\"examples_{shot}_shot_plus_bert.pickle\", \"rb\") as file:\n",
    "            examples = pickle.load(file)\n",
    "\n",
    "    for k in tqdm(definitions.keys()):\n",
    "        if k in examples.keys():\n",
    "            continue  # have already generated examples for this construction\n",
    "\n",
    "        definition = definitions[k]\n",
    "        try:\n",
    "            sentence = str(list(sentences[int(k)])[0:shot])  # get some sentences\n",
    "        except KeyError:\n",
    "            print((\"[]\", \"[]\", \"This seems wrong...\"))\n",
    "            examples[k] = (\"[]\", \"[]\")\n",
    "            continue\n",
    "\n",
    "        example = generate_and_check_examples(\n",
    "            definition, sentence, temperature=0.75,\n",
    "            max_new_tokens=1000, top_k=100, top_p=0.99\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            print((sentence, example))\n",
    "            examples[k] = (sentence, example)\n",
    "        except:\n",
    "            print((sentence, \"[]\"))\n",
    "            examples[k] = (sentence, \"[]\")\n",
    "\n",
    "        with open(f\"examples_{shot}_shot.pickle\", \"wb\") as file:\n",
    "            pickle.dump(examples, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322c70db2bca3a89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
