{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-26T12:51:05.819578300Z",
     "start_time": "2024-01-26T12:51:00.293677700Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertForNextSentencePrediction, BertTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['\"\"Was13',\n '\"647',\n '\"Wir-äh-spielen-äh-in-der-äh-Champions-League647',\n '(1597',\n '(1600',\n '(1602',\n '(1624',\n '(1637',\n '(1639',\n '(1641',\n '(1643',\n '(1645',\n '(379',\n '(579',\n '(581',\n '(584',\n '(590',\n '(592',\n '(600',\n '(886',\n '(889',\n '(892',\n '(900',\n '(905',\n '(907',\n '(909',\n '(911',\n '(917',\n '(919',\n '(921',\n '(923',\n ')1597',\n ')1600',\n ')1602',\n ')1624',\n ')1637',\n ')1639',\n ')1641',\n ')1643',\n ')1645',\n ')1792',\n ')379',\n ')579',\n ')581',\n ')584',\n ')590',\n ')592',\n ')600',\n ')886',\n ')889',\n ')892',\n ')900',\n ')905',\n ')907',\n ')909',\n ')911',\n ')917',\n ')919',\n ')921',\n ')923',\n ')«579',\n ',1459',\n ',973',\n '-128',\n '-651',\n '-654',\n '-875',\n '-973',\n ':595',\n ':875',\n ':973',\n 'Abstand683',\n 'Allein20',\n 'Aller1630',\n 'Als1315',\n 'Als133',\n 'Als1770',\n 'Am488',\n 'Am492',\n 'Am500',\n 'Amerika605',\n 'Anstatt320',\n 'Art129',\n 'Arzt1509',\n 'Augenblick1301',\n 'Ausmaß1777',\n 'BRUTAL1503',\n 'Besser1762',\n 'Bis559',\n 'Brutal1503',\n 'Buche1346',\n 'Das1313',\n 'Das1461',\n 'Dass21',\n 'Dasselbe104',\n 'Der1301',\n 'Desto111',\n 'Die1681',\n 'Diese19',\n 'Dieser1301',\n 'Dieser19',\n 'Dieses19',\n 'Entweder1779',\n 'Es1313',\n 'Es605',\n 'Gefühl1301',\n 'Generation1126',\n 'Geschweige10',\n 'Gleiche104',\n 'Gold1554',\n 'Gold1884',\n 'Hauptsache1033',\n 'Im557',\n 'In557',\n 'In858',\n 'Inbegriff1671',\n 'Ist1660',\n 'Je111',\n 'Jedermanns1329',\n 'Jener1301',\n 'Jetzt1631',\n 'Kein1525',\n 'Kein1715',\n 'Kein1835',\n 'Kein949',\n 'Keine(r1792',\n 'Keine1525',\n 'Keine1715',\n 'Keine1792',\n 'Keine1835',\n 'Keine902',\n 'Keine949',\n 'Keiner1792',\n 'Leben1461',\n 'Leben605',\n 'Lecker1649',\n 'Maß1777',\n 'Maße1777',\n 'Mit1029',\n 'Mit683',\n 'Moment1301',\n 'Mords651',\n 'Mutter1681',\n 'Neue875',\n 'Nicht1162',\n 'Nicht902',\n 'Niemand1792',\n 'Ponyhof1462',\n 'Raus1316',\n 'Reden1554',\n 'Riesen654',\n 'Silber1554',\n 'Silber1884',\n 'So1300',\n 'So1760',\n 'So1831',\n 'So22',\n 'So74',\n 'So98',\n 'Solch78',\n 'Solche78',\n 'Sowohl1134',\n 'Statt320',\n 'Tode1347',\n 'Trotz1630',\n 'Umfang1777',\n 'Umso111',\n 'Und11',\n 'Und5',\n 'Von1034',\n 'Von1629',\n 'Von1772',\n 'Warum1029',\n 'Was1029',\n 'Was13',\n 'Was16',\n 'Was1846',\n 'Weder12',\n 'Weil674',\n 'Weil681',\n 'Welch15',\n 'Wem1029',\n 'Wenn1291',\n 'Wer1029',\n 'Wie1029',\n 'Wie14',\n 'Wie1738',\n 'Wo1029',\n 'Wo976',\n 'Während320',\n 'Zeit605',\n 'Zeiten605',\n '[1643',\n ']1643',\n 'ab1574',\n 'aber902',\n 'allen1630',\n 'aller1630',\n 'aller1681',\n 'aller758',\n 'als1134',\n 'als133',\n 'als1762',\n 'als1770',\n 'als97',\n 'am488',\n 'am492',\n 'am500',\n 'an349',\n 'anstatt320',\n 'anstatt882',\n 'artig128',\n 'auch1134',\n 'auch1715',\n 'auch1835',\n 'auf1574',\n 'auf350',\n 'aufhübschen1029',\n 'aus1316',\n 'ausbreiten1029',\n 'auslöste1029',\n 'badete1029',\n 'bedeutet1029',\n 'besser1762',\n 'betreffen1029',\n 'bis1509',\n 'bis559',\n 'bisschen762',\n 'bleibt1660',\n 'braucht1029',\n 'bringt1029',\n 'brutal1503',\n 'darstellen1029',\n 'das104',\n 'das1301',\n 'das1459',\n 'das1461',\n 'das875',\n 'dass135',\n 'dasselbe104',\n 'dem1777',\n 'den557',\n 'den671',\n 'denn10',\n 'der1509',\n 'der1681',\n 'der557',\n 'der875',\n 'desto111',\n 'die1681',\n 'die875',\n 'diese605',\n 'dieses19',\n 'durch1574',\n 'ebenso122',\n 'ebenso98',\n 'ein127',\n 'ein1715',\n 'ein1835',\n 'ein762',\n 'einander1574',\n 'eine129',\n 'eine1715',\n 'eine1835',\n 'eine605',\n 'einem127',\n 'einer127',\n 'entweder1779',\n 'er1004',\n 'er1346',\n 'er99',\n 'erst5',\n 'erzeugt1029',\n 'es125',\n 'es1313',\n 'es1346',\n 'es1631',\n 'es605',\n 'euch1574',\n 'excellence1342',\n 'fahre1029',\n 'früher1459',\n 'für13',\n 'für1525',\n 'für350',\n 'gab605',\n 'ganz697',\n 'ganz777',\n 'gar5',\n 'gar697',\n 'gar777',\n 'geben605',\n 'gegeben605',\n 'gegen83',\n 'gehen1029',\n 'geht1300',\n 'genauso122',\n 'genauso98',\n 'geschweige10',\n 'gewesen1459',\n 'gibt605',\n 'gleich100',\n 'gleich675',\n 'gleich676',\n 'gleich98',\n 'gleiche104',\n 'gleichen1777',\n 'gleichkam676',\n 'gleichkommen676',\n 'gleichkommt676',\n 'gleichkäme676',\n 'gleicht132',\n 'gold1554',\n 'habe605',\n 'haben1029',\n 'haben605',\n 'hat605',\n 'hatte605',\n 'heisst1631',\n 'her1351',\n 'her1511',\n 'herunter1574',\n 'heute1459',\n 'hin1351',\n 'hin1511',\n 'hinaus1574',\n 'hindurch1574',\n 'hinunter1574',\n 'hätte1756',\n 'hätten1756',\n 'hättest1756',\n 'im1346',\n 'im1777',\n 'im557',\n 'in1316',\n 'in1777',\n 'in557',\n 'in605',\n 'in858',\n 'irrte1029',\n 'isst1029',\n 'ist1029',\n 'ist125',\n 'ist130',\n 'ist1313',\n 'ist1459',\n 'ist1461',\n 'ist1462',\n 'ist1554',\n 'ist1660',\n 'ist1715',\n 'ist1792',\n 'ist1835',\n 'ist1846',\n 'ist1881',\n 'ist1884',\n 'ist875',\n 'ist976',\n 'je111',\n 'jedermanns1329',\n 'jetzt1631',\n 'kam676',\n 'kann1593',\n 'kaum11',\n 'kaum1986',\n 'kein1461',\n 'kein1462',\n 'kein1525',\n 'kein1690',\n 'kein1715',\n 'kein1881',\n 'kein902',\n 'kein949',\n 'keine1461',\n 'keine1525',\n 'keine1715',\n 'keine1835',\n 'keine1881',\n 'keine605',\n 'keine902',\n 'keine949',\n 'keiner1792',\n 'kleiden1029',\n 'kommen1509',\n 'kommen676',\n 'kommt1509',\n 'kommt676',\n 'kostet1029',\n 'käme676',\n 'lachen1029',\n 'lassen1029',\n 'lassen1289',\n 'lebte1029',\n 'lecker1649',\n 'ließ1289',\n 'lässt1289',\n 'läuft1029',\n 'macht1029',\n 'machte1029',\n 'mag605',\n 'meinem605',\n 'meisten488',\n 'meisten492',\n 'mich1573',\n 'mich1574',\n 'mir1582',\n 'mit1846',\n 'mit683',\n 'mords651',\n 'muss1300',\n 'musste1509',\n 'müssen1029',\n 'neue875',\n 'neuen875',\n 'nicht.“1779',\n 'nicht1162',\n 'nicht1690',\n 'nicht1779',\n 'nicht5',\n 'nicht902',\n 'nicht904',\n 'niemand1525',\n 'niemand1792',\n 'noch12',\n 'noch1770',\n 'nämliche104',\n 'nützt1029',\n 'ob1315',\n 'ob133',\n 'oder1162',\n 'oder1779',\n 'oder973',\n 'ohne1690',\n 'ohne949',\n 'ohnegleichen618',\n 'par1342',\n 'pur.»1323',\n 'pur1323',\n 'pur«1323',\n 'raus1316',\n 'recht5',\n 'redet1029',\n 'rein1316',\n 'ruiniert1029',\n 'satt1324',\n 'schlechthin1556',\n 'schlechthin»1556',\n 'schon5',\n 'schufen1029',\n 'schützt1029',\n 'sehr1987',\n 'sei125',\n 'seien1660',\n 'sein1289',\n 'selben1777',\n 'sich1573',\n 'sich1574',\n 'sich1582',\n 'sie1346',\n 'silber1554',\n 'sind1029',\n 'sind130',\n 'sind1459',\n 'sind1554',\n 'sind1660',\n 'sind1881',\n 'sind875',\n 'sind976',\n 'so1134',\n 'so135',\n 'so1760',\n 'so1831',\n 'so74',\n 'so98',\n 'solch78',\n 'soll605',\n 'sollten1029',\n 'sondergleichen618',\n 'sondergleichen»618',\n 'sondern902',\n 'sowohl1134',\n 'stand1346',\n 'statt320',\n 'statt882',\n 'steht1346',\n 'um350',\n 'umso111',\n 'und1029',\n 'und11',\n 'und1140',\n 'und1849',\n 'und5',\n 'und697',\n 'und777',\n 'und904',\n 'uns1574',\n 'uns1582',\n 'unter671',\n 'verdient1029',\n 'verändern1029',\n 'von1034',\n 'von1035',\n 'von127',\n 'von1629',\n 'von1772',\n 'vor559',\n 'war1313',\n 'war1459',\n 'war1461',\n 'war1462',\n 'war1660',\n 'war1770',\n 'war1846',\n 'war1881',\n 'waren1459',\n 'waren1770',\n 'waren1881',\n 'was1029',\n 'was13',\n 'was1459',\n 'was1846',\n 'weder12',\n 'wegen1629',\n 'wegen1772',\n 'weil674',\n 'weil681',\n 'welch15',\n 'wem1029',\n 'wenig762',\n 'wenigsten488',\n 'wenigsten492',\n 'wenn1291',\n 'wenn136',\n 'werden1029',\n 'wie101',\n 'wie103',\n 'wie125',\n 'wie130',\n 'wie1346',\n 'wie136',\n 'wie1738',\n 'will1029',\n 'wird1029',\n 'wird1660',\n 'wird605',\n 'wissen1029',\n 'wo1029',\n 'wogegen320',\n 'wohingegen320',\n 'wohl11',\n 'wohl1134',\n 'wollen1029',\n 'wäre130',\n 'wäre1660',\n 'wäre1756']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudowords = []\n",
    "for i in range(15):\n",
    "    pseudowords.append(np.load(f\"../../data/pseudowords/bsbbert/pseudowords_comapp_bsbbert_{i*37}_{i*37+37}.npy\"))\n",
    "pseudowords = np.concatenate(pseudowords)\n",
    "\n",
    "csv_data = []\n",
    "for i in range(1, 16):\n",
    "    csv_data.append(pd.read_csv(f\"../../data/pseudowords/bsbbert/order_bsbbert_{i}.csv\", sep=\";\", index_col=0, header=None, quotechar=\"|\", names=[\"order\", \"label\"]))\n",
    "csv_data = pd.concat(csv_data)\n",
    "\n",
    "bert_tokens = [d[0] for d in csv_data.values]\n",
    "bert_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T12:51:05.840831900Z",
     "start_time": "2024-01-26T12:51:05.799289500Z"
    }
   },
   "id": "c395ff268451a4b2",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForNextSentencePrediction were not initialized from the model checkpoint at dbmdz/bert-base-german-cased and are newly initialized: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 31657. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForNextSentencePrediction(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(31657, 768)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (cls): BertOnlyNSPHead(\n    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained(\"dbmdz/bert-base-german-cased\", return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "\n",
    "combined_embeddings = torch.cat((model.bert.embeddings.word_embeddings.weight, torch.tensor(pseudowords)), dim=0)\n",
    "model.bert.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(combined_embeddings)\n",
    "tokenizer.add_tokens(bert_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T12:51:09.737470400Z",
     "start_time": "2024-01-26T12:51:05.831074400Z"
    }
   },
   "id": "737979872a7f76f1",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../out/definitions.pickle\", \"rb\") as file:\n",
    "    definitions = pickle.load(file)\n",
    "with open(\"../../out/sentences.pickle\", \"rb\") as file:\n",
    "    sentences = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T12:51:09.748300400Z",
     "start_time": "2024-01-26T12:51:09.739691Z"
    }
   },
   "id": "eb7917f993cd42d8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_examples(definition, examples, true_examples=0):\n",
    "    predictions = {}\n",
    "    tok_examples = [tokenizer.tokenize(example) for example in examples]\n",
    "    tok_eg = tokenizer.tokenize(\"zum Beispiel:\")\n",
    "    tok_definition = tokenizer.tokenize(definition[:definition.index(\".\")])  # only the first sentence of the definition is used, so BERT has a chance of keeping some of the definition in mind\n",
    "    for num, tok_example in enumerate(tok_examples):\n",
    "        len_prompt = len(tok_definition) + len(tok_eg) + len(tok_example) + 3  # 3 extra tokens for [CLS] and [SEP] (2x)\n",
    "        if len_prompt > 512:\n",
    "            # shorten the definition so that the example fits fully, and add \"...,\" (again, 4 additional tokens)\n",
    "            prompt = tokenizer.convert_tokens_to_string(tok_definition[:512-len(tok_example)-len(tok_eg)-3-4]) + \"..., zum Beispiel:\"\n",
    "        else:\n",
    "            prompt = tokenizer.convert_tokens_to_string(tok_definition) + \", zum Beispiel:\"\n",
    "            \n",
    "        inputs = tokenizer(prompt, tokenizer.convert_tokens_to_string(tok_example), return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if true_examples == 0:\n",
    "            predictions[num] = logits[0, 0] > logits[0, 1]  # next sentence is not random\n",
    "        else:\n",
    "            predictions[num] = logits[0, 0]  # collect in order to sort later\n",
    "    if true_examples == 0:\n",
    "        res = [examples[num] for num, p in predictions.items() if p]  # return all sentences which have been classified as correct\n",
    "    else:\n",
    "        sorted_idxs = sorted(range(len(examples)), key=lambda i: predictions[i], reverse=True)[:true_examples]\n",
    "        res = [examples[i] for i in sorted_idxs]\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T12:51:09.762701Z",
     "start_time": "2024-01-26T12:51:09.754598600Z"
    }
   },
   "id": "693ad2b73bd9bcb",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_metrics(positive_predicted, negative_predicted, true_sentences, false_sentences, key, definition, examples):\n",
    "    true_positives = [pr for pr in positive_predicted if pr in true_sentences]\n",
    "    false_positives = [pr for pr in positive_predicted if pr in false_sentences]\n",
    "    false_negatives = [pr for pr in negative_predicted if pr in true_sentences]\n",
    "    true_negatives = [pr for pr in negative_predicted if pr in false_sentences]\n",
    "    \n",
    "    if len(true_positives) + len(false_positives) > 0:\n",
    "        precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "    else:\n",
    "        precision = 1.0  # nothing found, so all things found are correct\n",
    "    \n",
    "    if len(true_positives) > 0:\n",
    "        recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "    else:\n",
    "        recall = 1.0  # all found\n",
    "        \n",
    "    return pd.Series({\n",
    "        \"constr\": key, \n",
    "        \"definition\": definition, \n",
    "        \"examples\": examples, \n",
    "        \"positive_predicted\": positive_predicted,\n",
    "        \"negative_predicted\": negative_predicted,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"true_negatives\": true_negatives,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": (2 * precision * recall) / (precision + recall),\n",
    "        \"accuracy\": (len(true_positives) + len(true_negatives)) / (len(true_sentences) + len(false_sentences))\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T13:25:23.988192700Z",
     "start_time": "2024-01-26T13:25:23.982058Z"
    }
   },
   "id": "9b56b8577f886471",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/211 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c95547aa69347efbb17cc4e5b06b563"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/211 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c30d82131664752aeff4a0eb77b60b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fcbd75784c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tim/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m false_sentences \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(random\u001B[38;5;241m.\u001B[39mchoices(others, k\u001B[38;5;241m=\u001B[39mnum_false))\n\u001B[1;32m     21\u001B[0m examples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(false_sentences \u001B[38;5;241m|\u001B[39m true_sentences)\n\u001B[0;32m---> 23\u001B[0m positive_predicted \u001B[38;5;241m=\u001B[39m \u001B[43mfind_examples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefinition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexamples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m negative_predicted \u001B[38;5;241m=\u001B[39m [ex \u001B[38;5;28;01mfor\u001B[39;00m ex \u001B[38;5;129;01min\u001B[39;00m examples \u001B[38;5;28;01mif\u001B[39;00m ex \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m positive_predicted]\n\u001B[1;32m     26\u001B[0m result\u001B[38;5;241m.\u001B[39mappend(get_metrics(positive_predicted, negative_predicted, true_sentences, false_sentences, key, definition, examples))\n",
      "Cell \u001B[0;32mIn[10], line 16\u001B[0m, in \u001B[0;36mfind_examples\u001B[0;34m(definition, examples, true_examples)\u001B[0m\n\u001B[1;32m     14\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tokenizer(prompt, tokenizer\u001B[38;5;241m.\u001B[39mconvert_tokens_to_string(tok_example), return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 16\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m true_examples \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1479\u001B[0m, in \u001B[0;36mBertForNextSentencePrediction.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m   1475\u001B[0m     labels \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_sentence_label\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1477\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1479\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1480\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1482\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1483\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1484\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1485\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1486\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1487\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1488\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1491\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   1493\u001B[0m seq_relationship_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls(pooled_output)\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1004\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m   1006\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1007\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1008\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1011\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m   1012\u001B[0m )\n\u001B[0;32m-> 1013\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1025\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1026\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    596\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    597\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    598\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    604\u001B[0m         output_attentions,\n\u001B[1;32m    605\u001B[0m     )\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 607\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    617\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    486\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    487\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    494\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    495\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    496\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 497\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    419\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    425\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    426\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 427\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    436\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    437\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:311\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    308\u001B[0m     key_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey(hidden_states))\n\u001B[1;32m    309\u001B[0m     value_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue(hidden_states))\n\u001B[0;32m--> 311\u001B[0m query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose_for_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmixed_query_layer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m use_cache \u001B[38;5;241m=\u001B[39m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_decoder:\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;66;03m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001B[39;00m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;66;03m# Further calls to cross_attention layer can then reuse all cross-attention\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;66;03m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001B[39;00m\n\u001B[1;32m    321\u001B[0m     \u001B[38;5;66;03m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001B[39;00m\n",
      "File \u001B[0;32m~/Projekte/llm-x-cxg/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:273\u001B[0m, in \u001B[0;36mBertSelfAttention.transpose_for_scores\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranspose_for_scores\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    272\u001B[0m     new_x_shape \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_attention_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_head_size)\n\u001B[0;32m--> 273\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_x_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(15)\n",
    "attempts = 15\n",
    "for num_true in range(0, 6):\n",
    "    for num_false in range(0, 6):\n",
    "        if not num_true and not num_false:\n",
    "            continue  # skip (0, 0)\n",
    "        result = []\n",
    "        for key, definition in tqdm(definitions.items()):\n",
    "            others = list(itertools.chain.from_iterable([sentence_list for constr, sentence_list in sentences.items() if int(constr) != int(key)]))  # flatten all other sentences which are not part of the current construction\n",
    "            \n",
    "            for attempt in range(attempts):\n",
    "                try:\n",
    "                    # pick the true elements of the current construction\n",
    "                    true_sentences = {random.choice(list(sentences[int(key)])) for t in range(num_true)}\n",
    "                except KeyError:\n",
    "                    result.append(pd.Series({\"constr\": key, \"definition\": definition}))\n",
    "                    continue\n",
    "    \n",
    "                # pick random false positives from the other sentences\n",
    "                false_sentences = set(random.choices(others, k=num_false))\n",
    "                examples = list(false_sentences | true_sentences)\n",
    "                \n",
    "                positive_predicted = find_examples(definition, examples)\n",
    "                negative_predicted = [ex for ex in examples if ex not in positive_predicted]\n",
    "                \n",
    "                result.append(get_metrics(positive_predicted, negative_predicted, true_sentences, false_sentences, key, definition, examples))\n",
    "                \n",
    "        result = pd.DataFrame(result)\n",
    "        result.to_csv(f\"../../out/comapp/result_{num_true}t_vs_{num_false}f_{attempts}attempts_bsbbert.tsv\", sep=\"\\t\", decimal=\",\", header=True, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T22:44:24.242782420Z",
     "start_time": "2024-01-25T22:42:49.487799661Z"
    }
   },
   "id": "ecad5aeec67bfd80",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{5: {'Und5', 'erst5', 'gar5', 'nicht5', 'recht5', 'schon5', 'und5'},\n 10: {'Geschweige10', 'denn10', 'geschweige10'},\n 11: {'Und11', 'kaum11', 'und11', 'wohl11'},\n 12: {'Weder12', 'noch12', 'weder12'},\n 13: {'\"\"Was13', 'Was13', 'für13', 'was13'},\n 14: {'Wie14'},\n 15: {'Welch15', 'welch15'},\n 16: {'Was16'},\n 19: {'Diese19', 'Dieser19', 'Dieses19', 'dieses19'},\n 20: {'Allein20'},\n 21: {'Dass21'},\n 22: {'So22'},\n 74: {'So74', 'so74'},\n 78: {'Solch78', 'Solche78', 'solch78'},\n 83: {'gegen83'},\n 97: {'als97'},\n 98: {'So98', 'ebenso98', 'genauso98', 'gleich98', 'so98'},\n 99: {'er99'},\n 100: {'gleich100'},\n 101: {'wie101'},\n 103: {'wie103'},\n 104: {'Dasselbe104',\n  'Gleiche104',\n  'das104',\n  'dasselbe104',\n  'gleiche104',\n  'nämliche104'},\n 111: {'Desto111', 'Je111', 'Umso111', 'desto111', 'je111', 'umso111'},\n 122: {'ebenso122', 'genauso122'},\n 125: {'es125', 'ist125', 'sei125', 'wie125'},\n 127: {'ein127', 'einem127', 'einer127', 'von127'},\n 128: {'-128', 'artig128'},\n 129: {'Art129', 'eine129'},\n 130: {'ist130', 'sind130', 'wie130', 'wäre130'},\n 132: {'gleicht132'},\n 133: {'Als133', 'als133', 'ob133'},\n 135: {'dass135', 'so135'},\n 136: {'wenn136', 'wie136'},\n 320: {'Anstatt320',\n  'Statt320',\n  'Während320',\n  'anstatt320',\n  'statt320',\n  'wogegen320',\n  'wohingegen320'},\n 349: {'an349'},\n 350: {'auf350', 'für350', 'um350'},\n 379: {'(379', ')379'},\n 488: {'Am488', 'am488', 'meisten488', 'wenigsten488'},\n 492: {'Am492', 'am492', 'meisten492', 'wenigsten492'},\n 500: {'Am500', 'am500'},\n 557: {'Im557', 'In557', 'den557', 'der557', 'im557', 'in557'},\n 559: {'Bis559', 'bis559', 'vor559'},\n 579: {'(579', ')579', ')«579'},\n 581: {'(581', ')581'},\n 584: {'(584', ')584'},\n 590: {'(590', ')590'},\n 592: {'(592', ')592'},\n 595: {':595'},\n 600: {'(600', ')600'},\n 605: {'Amerika605',\n  'Es605',\n  'Leben605',\n  'Zeit605',\n  'Zeiten605',\n  'diese605',\n  'eine605',\n  'es605',\n  'gab605',\n  'geben605',\n  'gegeben605',\n  'gibt605',\n  'habe605',\n  'haben605',\n  'hat605',\n  'hatte605',\n  'in605',\n  'keine605',\n  'mag605',\n  'meinem605',\n  'soll605',\n  'wird605'},\n 618: {'ohnegleichen618', 'sondergleichen618', 'sondergleichen»618'},\n 647: {'\"647', '\"Wir-äh-spielen-äh-in-der-äh-Champions-League647'},\n 651: {'-651', 'Mords651', 'mords651'},\n 654: {'-654', 'Riesen654'},\n 671: {'den671', 'unter671'},\n 674: {'Weil674', 'weil674'},\n 675: {'gleich675'},\n 676: {'gleich676',\n  'gleichkam676',\n  'gleichkommen676',\n  'gleichkommt676',\n  'gleichkäme676',\n  'kam676',\n  'kommen676',\n  'kommt676',\n  'käme676'},\n 681: {'Weil681', 'weil681'},\n 683: {'Abstand683', 'Mit683', 'mit683'},\n 697: {'ganz697', 'gar697', 'und697'},\n 758: {'aller758'},\n 762: {'bisschen762', 'ein762', 'wenig762'},\n 777: {'ganz777', 'gar777', 'und777'},\n 858: {'In858', 'in858'},\n 875: {'-875',\n  ':875',\n  'Neue875',\n  'das875',\n  'der875',\n  'die875',\n  'ist875',\n  'neue875',\n  'neuen875',\n  'sind875'},\n 882: {'anstatt882', 'statt882'},\n 886: {'(886', ')886'},\n 889: {'(889', ')889'},\n 892: {'(892', ')892'},\n 900: {'(900', ')900'},\n 902: {'Keine902',\n  'Nicht902',\n  'aber902',\n  'kein902',\n  'keine902',\n  'nicht902',\n  'sondern902'},\n 904: {'nicht904', 'und904'},\n 905: {'(905', ')905'},\n 907: {'(907', ')907'},\n 909: {'(909', ')909'},\n 911: {'(911', ')911'},\n 917: {'(917', ')917'},\n 919: {'(919', ')919'},\n 921: {'(921', ')921'},\n 923: {'(923', ')923'},\n 949: {'Kein949', 'Keine949', 'kein949', 'keine949', 'ohne949'},\n 973: {',973', '-973', ':973', 'oder973'},\n 976: {'Wo976', 'ist976', 'sind976'},\n 1004: {'er1004'},\n 1029: {'Mit1029',\n  'Warum1029',\n  'Was1029',\n  'Wem1029',\n  'Wer1029',\n  'Wie1029',\n  'Wo1029',\n  'aufhübschen1029',\n  'ausbreiten1029',\n  'auslöste1029',\n  'badete1029',\n  'bedeutet1029',\n  'betreffen1029',\n  'braucht1029',\n  'bringt1029',\n  'darstellen1029',\n  'erzeugt1029',\n  'fahre1029',\n  'gehen1029',\n  'haben1029',\n  'irrte1029',\n  'isst1029',\n  'ist1029',\n  'kleiden1029',\n  'kostet1029',\n  'lachen1029',\n  'lassen1029',\n  'lebte1029',\n  'läuft1029',\n  'macht1029',\n  'machte1029',\n  'müssen1029',\n  'nützt1029',\n  'redet1029',\n  'ruiniert1029',\n  'schufen1029',\n  'schützt1029',\n  'sind1029',\n  'sollten1029',\n  'und1029',\n  'verdient1029',\n  'verändern1029',\n  'was1029',\n  'wem1029',\n  'werden1029',\n  'will1029',\n  'wird1029',\n  'wissen1029',\n  'wo1029',\n  'wollen1029'},\n 1033: {'Hauptsache1033'},\n 1034: {'Von1034', 'von1034'},\n 1035: {'von1035'},\n 1126: {'Generation1126'},\n 1134: {'Sowohl1134',\n  'als1134',\n  'auch1134',\n  'so1134',\n  'sowohl1134',\n  'wohl1134'},\n 1140: {'und1140'},\n 1162: {'Nicht1162', 'nicht1162', 'oder1162'},\n 1289: {'lassen1289', 'ließ1289', 'lässt1289', 'sein1289'},\n 1291: {'Wenn1291', 'wenn1291'},\n 1300: {'So1300', 'geht1300', 'muss1300'},\n 1301: {'Augenblick1301',\n  'Der1301',\n  'Dieser1301',\n  'Gefühl1301',\n  'Jener1301',\n  'Moment1301',\n  'das1301'},\n 1313: {'Das1313', 'Es1313', 'es1313', 'ist1313', 'war1313'},\n 1315: {'Als1315', 'ob1315'},\n 1316: {'Raus1316', 'aus1316', 'in1316', 'raus1316', 'rein1316'},\n 1323: {'pur.»1323', 'pur1323', 'pur«1323'},\n 1324: {'satt1324'},\n 1329: {'Jedermanns1329', 'jedermanns1329'},\n 1342: {'excellence1342', 'par1342'},\n 1346: {'Buche1346',\n  'er1346',\n  'es1346',\n  'im1346',\n  'sie1346',\n  'stand1346',\n  'steht1346',\n  'wie1346'},\n 1347: {'Tode1347'},\n 1351: {'her1351', 'hin1351'},\n 1459: {',1459',\n  'das1459',\n  'früher1459',\n  'gewesen1459',\n  'heute1459',\n  'ist1459',\n  'sind1459',\n  'war1459',\n  'waren1459',\n  'was1459'},\n 1461: {'Das1461',\n  'Leben1461',\n  'das1461',\n  'ist1461',\n  'kein1461',\n  'keine1461',\n  'war1461'},\n 1462: {'Ponyhof1462', 'ist1462', 'kein1462', 'war1462'},\n 1503: {'BRUTAL1503', 'Brutal1503', 'brutal1503'},\n 1509: {'Arzt1509',\n  'bis1509',\n  'der1509',\n  'kommen1509',\n  'kommt1509',\n  'musste1509'},\n 1511: {'her1511', 'hin1511'},\n 1525: {'Kein1525',\n  'Keine1525',\n  'für1525',\n  'kein1525',\n  'keine1525',\n  'niemand1525'},\n 1554: {'Gold1554',\n  'Reden1554',\n  'Silber1554',\n  'gold1554',\n  'ist1554',\n  'silber1554',\n  'sind1554'},\n 1556: {'schlechthin1556', 'schlechthin»1556'},\n 1573: {'mich1573', 'sich1573'},\n 1574: {'ab1574',\n  'auf1574',\n  'durch1574',\n  'einander1574',\n  'euch1574',\n  'herunter1574',\n  'hinaus1574',\n  'hindurch1574',\n  'hinunter1574',\n  'mich1574',\n  'sich1574',\n  'uns1574'},\n 1582: {'mir1582', 'sich1582', 'uns1582'},\n 1593: {'kann1593'},\n 1597: {'(1597', ')1597'},\n 1600: {'(1600', ')1600'},\n 1602: {'(1602', ')1602'},\n 1624: {'(1624', ')1624'},\n 1629: {'Von1629', 'von1629', 'wegen1629'},\n 1630: {'Aller1630', 'Trotz1630', 'allen1630', 'aller1630'},\n 1631: {'Jetzt1631', 'es1631', 'heisst1631', 'jetzt1631'},\n 1637: {'(1637', ')1637'},\n 1639: {'(1639', ')1639'},\n 1641: {'(1641', ')1641'},\n 1643: {'(1643', ')1643', '[1643', ']1643'},\n 1645: {'(1645', ')1645'},\n 1649: {'Lecker1649', 'lecker1649'},\n 1660: {'Ist1660',\n  'bleibt1660',\n  'ist1660',\n  'seien1660',\n  'sind1660',\n  'war1660',\n  'wird1660',\n  'wäre1660'},\n 1671: {'Inbegriff1671'},\n 1681: {'Die1681', 'Mutter1681', 'aller1681', 'der1681', 'die1681'},\n 1690: {'kein1690', 'nicht1690', 'ohne1690'},\n 1715: {'Kein1715',\n  'Keine1715',\n  'auch1715',\n  'ein1715',\n  'eine1715',\n  'ist1715',\n  'kein1715',\n  'keine1715'},\n 1738: {'Wie1738', 'wie1738'},\n 1756: {'hätte1756', 'hätten1756', 'hättest1756', 'wäre1756'},\n 1760: {'So1760', 'so1760'},\n 1762: {'Besser1762', 'als1762', 'besser1762'},\n 1770: {'Als1770', 'als1770', 'noch1770', 'war1770', 'waren1770'},\n 1772: {'Von1772', 'von1772', 'wegen1772'},\n 1777: {'Ausmaß1777',\n  'Maß1777',\n  'Maße1777',\n  'Umfang1777',\n  'dem1777',\n  'gleichen1777',\n  'im1777',\n  'in1777',\n  'selben1777'},\n 1779: {'Entweder1779',\n  'entweder1779',\n  'nicht.“1779',\n  'nicht1779',\n  'oder1779'},\n 1792: {')1792',\n  'Keine(r1792',\n  'Keine1792',\n  'Keiner1792',\n  'Niemand1792',\n  'ist1792',\n  'keiner1792',\n  'niemand1792'},\n 1831: {'So1831', 'so1831'},\n 1835: {'Kein1835',\n  'Keine1835',\n  'auch1835',\n  'ein1835',\n  'eine1835',\n  'ist1835',\n  'keine1835'},\n 1846: {'Was1846', 'ist1846', 'mit1846', 'war1846', 'was1846'},\n 1849: {'und1849'},\n 1881: {'ist1881',\n  'kein1881',\n  'keine1881',\n  'sind1881',\n  'war1881',\n  'waren1881'},\n 1884: {'Gold1884', 'Silber1884', 'ist1884'},\n 1986: {'kaum1986'},\n 1987: {'sehr1987'}}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kelex = csv_data.copy()\n",
    "kelex['constr'] = csv_data['label'].str.extract('(\\d+)').astype(int)\n",
    "#kelex.set_index('constr', inplace=True)\n",
    "kelex = kelex.groupby('constr')['label'].apply(set).to_dict()\n",
    "kelex"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T12:51:16.915023900Z",
     "start_time": "2024-01-26T12:51:16.894308500Z"
    }
   },
   "id": "cd5df0ed9a88a378",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/211 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93544cca1d7a41118dd4c9674735c2d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 42\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m true_sentences \u001B[38;5;129;01min\u001B[39;00m true_sentences_kelex:\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(true_sentences) \u001B[38;5;241m==\u001B[39m num_true:  \u001B[38;5;66;03m# watch out that the number of true_sentences is still coherent with the number predefined by num_true (e.g. in case a specific KE-lex wasn't in the sentence)\u001B[39;00m\n\u001B[0;32m     41\u001B[0m         \u001B[38;5;66;03m# pick random false positives from the other sentences                    \u001B[39;00m\n\u001B[1;32m---> 42\u001B[0m         false_sentences \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(random\u001B[38;5;241m.\u001B[39mchoices(others, k\u001B[38;5;241m=\u001B[39mnum_false))\n\u001B[0;32m     43\u001B[0m         examples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(false_sentences \u001B[38;5;241m|\u001B[39m true_sentences)\n\u001B[0;32m     45\u001B[0m         positive_predicted \u001B[38;5;241m=\u001B[39m find_examples(definition, examples)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(15)\n",
    "attempts = 15\n",
    "for num_true in range(1, 6):\n",
    "    for num_false in range(0, 6):\n",
    "        if not num_true and not num_false:\n",
    "            continue  # skip (0, 0)\n",
    "        result = []\n",
    "        for key, definition in tqdm(definitions.items()):\n",
    "            others = list(itertools.chain.from_iterable([sentence_list for constr, sentence_list in sentences.items() if int(constr) != int(key)]))\n",
    "            for attempt in range(attempts):\n",
    "                try:\n",
    "                    # pick the true elements of the current construction\n",
    "                    true_sentences = {random.choice(list(sentences[int(key)])) for t in range(num_true)}\n",
    "                except KeyError:\n",
    "                    result.append(pd.Series({\"constr\": key, \"definition\": definition}))\n",
    "                    continue\n",
    "                    \n",
    "                true_sentences_kelex = []\n",
    "                if kelex.get(key):  # if there is a predefined KE-lex in the construction\n",
    "                    # if len(true_sentences) > 0:\n",
    "                        for pseudoword in kelex[key]:  # replace each predefined KE-lex in the sentence one-by-one (if there are multiple)\n",
    "                            cur_true_sentences = []\n",
    "                            for sentence in true_sentences:\n",
    "                                pseudoword_found = False\n",
    "                                sentence_kelex = []\n",
    "                                for token in sentence.split():\n",
    "                                    new_token = token\n",
    "                                    if token == re.findall(r'\\D+', pseudoword)[0]:\n",
    "                                        new_token = pseudoword\n",
    "                                        pseudoword_found = True\n",
    "                                    sentence_kelex.append(new_token)\n",
    "                                if pseudoword_found:  # only add the sentence if it has a replaced pseudoword\n",
    "                                    cur_true_sentences.append(\" \".join(sentence_kelex))\n",
    "                            #if len(cur_true_sentences) > 0:\n",
    "                            true_sentences_kelex.append(set(cur_true_sentences))\n",
    "                else:\n",
    "                    continue  # skip constructions without kelex\n",
    "                \n",
    "                for true_sentences in true_sentences_kelex:\n",
    "                    if len(true_sentences) == num_true:  # watch out that the number of true_sentences is still coherent with the number predefined by num_true (e.g. in case a specific KE-lex wasn't in the sentence)\n",
    "                        # pick random false positives from the other sentences                    \n",
    "                        false_sentences = set(random.choices(others, k=num_false))\n",
    "                        examples = list(false_sentences | true_sentences)\n",
    "                        \n",
    "                        positive_predicted = find_examples(definition, examples)\n",
    "                        negative_predicted = [ex for ex in examples if ex not in positive_predicted]\n",
    "                        \n",
    "                        result.append(get_metrics(positive_predicted, negative_predicted, true_sentences, false_sentences, key, definition, examples))\n",
    "                    \n",
    "                \n",
    "        result = pd.DataFrame(result)\n",
    "        result.to_csv(f\"../../out/comapp/result_{num_true}t_vs_{num_false}f_kelex_{attempts}attempts_bsbbert.tsv\", sep=\"\\t\", decimal=\",\", header=True, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T13:31:20.420592300Z",
     "start_time": "2024-01-26T13:30:56.778615300Z"
    }
   },
   "id": "cfdb3dc48dea6759",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/211 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23fda7e0837d4aaaa2eff578bbbbb337"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m         positive_predicted \u001B[38;5;241m=\u001B[39m find_examples(definition, examples, num_true)\n\u001B[0;32m     24\u001B[0m         negative_predicted \u001B[38;5;241m=\u001B[39m [ex \u001B[38;5;28;01mfor\u001B[39;00m ex \u001B[38;5;129;01min\u001B[39;00m examples \u001B[38;5;28;01mif\u001B[39;00m ex \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m positive_predicted]\n\u001B[1;32m---> 26\u001B[0m         result\u001B[38;5;241m.\u001B[39mappend(get_metrics(positive_predicted, negative_predicted, true_sentences, false_sentences, key, definition, examples))\n\u001B[0;32m     28\u001B[0m result \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(result)\n\u001B[0;32m     29\u001B[0m result\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../out/comapp/result_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_true\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mt_vs_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_false\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mf_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattempts\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mattempts_bsbbert_2.tsv\u001B[39m\u001B[38;5;124m\"\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, decimal\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(15)\n",
    "attempts = 15\n",
    "for num_true in range(1, 6):\n",
    "    for num_false in range(1, 6):\n",
    "        if not num_true and not num_false:\n",
    "            continue  # skip (0, 0)\n",
    "        result = []\n",
    "        for key, definition in tqdm(definitions.items()):\n",
    "            others = list(itertools.chain.from_iterable([sentence_list for constr, sentence_list in sentences.items() if int(constr) != int(key)]))  # flatten all other sentences which are not part of the current construction\n",
    "            \n",
    "            for attempt in range(attempts):\n",
    "                try:\n",
    "                    # pick the true elements of the current construction\n",
    "                    true_sentences = {random.choice(list(sentences[int(key)])) for t in range(num_true)}\n",
    "                except KeyError:\n",
    "                    result.append(pd.Series({\"constr\": key, \"definition\": definition}))\n",
    "                    continue\n",
    "    \n",
    "                # pick random false positives from the other sentences\n",
    "                false_sentences = set(random.choices(others, k=num_false))\n",
    "                examples = list(false_sentences | true_sentences)\n",
    "                \n",
    "                positive_predicted = find_examples(definition, examples, num_true)\n",
    "                negative_predicted = [ex for ex in examples if ex not in positive_predicted]\n",
    "                \n",
    "                result.append(get_metrics(positive_predicted, negative_predicted, true_sentences, false_sentences, key, definition, examples))\n",
    "                \n",
    "        result = pd.DataFrame(result)\n",
    "        result.to_csv(f\"../../out/comapp/result_{num_true}t_vs_{num_false}f_{attempts}attempts_bsbbert_2.tsv\", sep=\"\\t\", decimal=\",\", header=True, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T13:28:15.980699300Z",
     "start_time": "2024-01-26T13:26:28.676712800Z"
    }
   },
   "id": "43b177fa6acb3e53",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random.seed(15)\n",
    "attempts = 15\n",
    "for num_true in range(1, 6):\n",
    "    for num_false in range(1, 6):\n",
    "        result = []\n",
    "        for key, definition in tqdm(definitions.items()):\n",
    "            others = list(itertools.chain.from_iterable([sentence_list for constr, sentence_list in sentences.items() if int(constr) != int(key)]))\n",
    "            for attempt in range(attempts):\n",
    "                try:\n",
    "                    # pick the true elements of the current construction\n",
    "                    true_sentences = {random.choice(list(sentences[int(key)])) for t in range(num_true)}\n",
    "                except KeyError:\n",
    "                    result.append(pd.Series({\"constr\": key, \"definition\": definition}))\n",
    "                    continue\n",
    "                    \n",
    "                true_sentences_kelex = []\n",
    "                if kelex.get(key):  # if there is a predefined KE-lex in the construction\n",
    "                    # if len(true_sentences) > 0:\n",
    "                        for pseudoword in kelex[key]:  # replace each predefined KE-lex in the sentence one-by-one (if there are multiple)\n",
    "                            cur_true_sentences = []\n",
    "                            for sentence in true_sentences:\n",
    "                                pseudoword_found = False\n",
    "                                sentence_kelex = []\n",
    "                                for token in sentence.split():\n",
    "                                    new_token = token\n",
    "                                    if token == re.findall(r'\\D+', pseudoword)[0]:\n",
    "                                        new_token = pseudoword\n",
    "                                        pseudoword_found = True\n",
    "                                    sentence_kelex.append(new_token)\n",
    "                                if pseudoword_found:  # only add the sentence if it has a replaced pseudoword\n",
    "                                    cur_true_sentences.append(\" \".join(sentence_kelex))\n",
    "                            #if len(cur_true_sentences) > 0:\n",
    "                            true_sentences_kelex.append(set(cur_true_sentences))\n",
    "                else:\n",
    "                    continue  # skip constructions without kelex\n",
    "                \n",
    "                for true_sentences in true_sentences_kelex:\n",
    "                    if len(true_sentences) == num_true:  # watch out that the number of true_sentences is still coherent with the number predefined by num_true (e.g. in case a specific KE-lex wasn't in the sentence)\n",
    "                        # pick random false positives from the other sentences                    \n",
    "                        false_sentences = set(random.choices(others, k=num_false))\n",
    "                        examples = list(false_sentences | true_sentences)\n",
    "                        \n",
    "                        positive_predicted = find_examples(definition, examples, true_examples=num_true)\n",
    "                        negative_predicted = [ex for ex in examples if ex not in positive_predicted]\n",
    "                        \n",
    "                        result.append(get_metrics(positive_predicted, negative_predicted, true_sentences, false_sentences, key, definition, examples))\n",
    "                        \n",
    "                \n",
    "        result = pd.DataFrame(result)\n",
    "        result.to_csv(f\"../../out/comapp/result_{num_true}t_vs_{num_false}f_kelex_{attempts}attempts_bsbbert_2.tsv\", sep=\"\\t\", decimal=\",\", header=True, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T13:22:29.099405300Z",
     "start_time": "2024-01-26T13:22:29.094168600Z"
    }
   },
   "id": "58162688e6c23c1b",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
