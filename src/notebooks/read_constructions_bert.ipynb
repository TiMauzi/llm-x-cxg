{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121db90d59a1dfe4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data from the German FrameNet Constructicon for BERT processing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affedc260145e51e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_masked_sentences(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sentences_data = []\n",
    "    \n",
    "    constr_id = int(root.attrib.get('id'))\n",
    "    \n",
    "    for sentence in root.findall('.//sentence'):\n",
    "        if sentence.attrib.get('uid') is None:\n",
    "            continue\n",
    "        \n",
    "        uid = sentence.attrib.get('uid')\n",
    "        text = sentence.find('.//text').text.strip()\n",
    "        \n",
    "        text_pos = []\n",
    "        text_xpos = []\n",
    "        text_dep = []\n",
    "        text_head = []\n",
    "        kees = []\n",
    "        kees_idx = []\n",
    "        kes = []\n",
    "        kes_idx = []\n",
    "        \n",
    "        # Loop through annotations:\n",
    "        for layer in sentence.findall('.//layer'):\n",
    "            layer_name = layer.attrib.get('name')\n",
    "            \n",
    "            # Get the KEE annotations:\n",
    "            if \"KE-\" in layer_name or \"KEE-\" in layer_name:\n",
    "                # Loop over all KEEs or KEs:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    start = int(label.attrib.get('start'))\n",
    "                    end = int(label.attrib.get('end'))\n",
    "                    if \"KEE-\" in layer_name:\n",
    "                        kees.append(text[start:end])  # Read the KEE\n",
    "                        kees_idx.append((start, end))  # Log the position of the KEE\n",
    "                    else:\n",
    "                        kes.append(text[start:end])  # Read the KE\n",
    "                        kes_idx.append((start, end))  # Log the position of the KE\n",
    "            elif \"UPOS\" == layer_name:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    text_pos.append(label.attrib.get('name'))\n",
    "            elif \"XPOS\" == layer_name:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    text_xpos.append(label.attrib.get('name'))\n",
    "            elif \"DEP_REL\" == layer_name:\n",
    "                for label in layer.findall('.//label'):\n",
    "                    text_dep.append(label.attrib.get('name'))\n",
    "            elif \"DEP_HEAD\" == layer_name:\n",
    "                for label in layer.findall('../label'):\n",
    "                    text_head.append(label.attrib.get('name'))\n",
    "                    \n",
    "        sentences_data.append({\n",
    "            'uid': uid,\n",
    "            'constr_id': constr_id,\n",
    "            'text': text,\n",
    "            'text_pos': text_pos,\n",
    "            'text_xpos': text_xpos,\n",
    "            'text_dep': text_dep,\n",
    "            'text_head': text_head,\n",
    "            'kees': kees,\n",
    "            'kees_idx': kees_idx,\n",
    "            'kes': kes,\n",
    "            'kes_idx': kes_idx,\n",
    "        })\n",
    "    return sentences_data\n",
    "    \n",
    "sentence_list = []\n",
    "xml_directory = ('../../data/constructicon/construction')\n",
    "\n",
    "for filename in tqdm(list(os.listdir(xml_directory))):\n",
    "    if filename.endswith('.xml'):\n",
    "        constr_id = Path(filename).stem\n",
    "        if b\"fa-triangle-exclamation\" in requests.get(f\"https://gsw.phil.hhu.de/constructicon/construction?id={constr_id}\").content:\n",
    "            print(constr_id, \"does not exist online!\")\n",
    "            continue\n",
    "        xml_file = os.path.join(xml_directory, filename)\n",
    "        data = parse_masked_sentences(xml_file)\n",
    "        if data:\n",
    "            sentence_list += data\n",
    "        time.sleep(.5)\n",
    "\n",
    "sentences = pd.DataFrame.from_dict(sentence_list)\n",
    "# sentences.set_index('uid', inplace=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = sentences.explode([\"kees\", \"kees_idx\"], ignore_index=True)\n",
    "sentences = sentences.explode([\"kes\", \"kes_idx\"], ignore_index=True)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "955e8ad114b0d4db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_comapp = []\n",
    "csv_comapp = []\n",
    "errors = 0\n",
    "problematic_constructions = set()\n",
    "unproblematic_constructions = set()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "for _, row in tqdm(sentences.iterrows(), total=len(sentences)):\n",
    "    tokenized = str(row[\"text\"]).split()\n",
    "    \n",
    "    try:\n",
    "        ke_start, ke_end = row[\"kes_idx\"]\n",
    "        tokenized_kees = str(row[\"kees\"]).split()  # Split the KEEs if there are multi-word KEEs\n",
    "    except TypeError:\n",
    "        continue  # If there is nothing to mask or if there is no KEE, we can't use this example.\n",
    "    \n",
    "    tokenized_kes = str(row[\"text\"][ke_start:ke_end]).split()\n",
    "    \n",
    "    masked_texts = []\n",
    "    tokenized_masked_list = []\n",
    "    \n",
    "    for k, tokenized_ke in enumerate(tokenized_kes):\n",
    "        masked_text = (\n",
    "                row[\"text\"][:ke_start] + \" \"\n",
    "                + \" \".join(tokenized_kes[:k]) \n",
    "                + \" [MASK] \" \n",
    "                + \" \".join(tokenized_kes[k+1:]) \n",
    "                + \" \" + row[\"text\"][ke_end:]\n",
    "        ).replace(\"  \", \" \").replace(\"  \", \" \")\n",
    "        masked_texts.append(masked_text)\n",
    "        tokenized_masked_list.append(str(masked_text).split())\n",
    "        \n",
    "    for masked_text, tokenized_masked in zip(masked_texts, tokenized_masked_list):\n",
    "        try:\n",
    "            kee_idx = [tokenized.index(tokenized_kee) for tokenized_kee in tokenized_kees]\n",
    "            kee_query_idx = []\n",
    "            for i in kee_idx:\n",
    "                kee_query_idx.append(i)\n",
    "            assert len(masked_text.split()) == len(row[\"text\"].split())\n",
    "            \n",
    "        except (ValueError, AssertionError) as e:\n",
    "            print(row[\"constr_id\"], type(e), e, \"... Continuing ...\")\n",
    "            errors += 1\n",
    "            problematic_constructions.add(row[\"constr_id\"])\n",
    "            continue\n",
    "            \n",
    "        unproblematic_constructions.add(row[\"constr_id\"])\n",
    "\n",
    "        assert \"[MASK]\" in masked_text\n",
    "\n",
    "        out_json = [{\n",
    "            \"label\": kee + str(row[\"constr_id\"]),\n",
    "            \"target1\": row[\"text\"], \n",
    "            \"target1_idx\": idx, \n",
    "            \"query\": masked_text,\n",
    "            \"query_idx\": q\n",
    "        } for kee, idx, q in zip(tokenized_kees, kee_idx, kee_query_idx)]  # Split the KEEs if there are multi-word KEEs\n",
    "        out_csv = [{\n",
    "            \"text\": row[\"text\"],\n",
    "            \"pos_tags\": row[\"text_pos\"],\n",
    "            \"xpos_tags\": row[\"text_xpos\"],\n",
    "            \"dep_rels\": row[\"text_dep\"],\n",
    "            \"dep_heads\": row[\"text_head\"],\n",
    "            \"mask\": row[\"text\"][ke_start:ke_end],\n",
    "            \"ambiguous_word\": kee,\n",
    "            \"label\": kee + str(row[\"constr_id\"])  # This will be the new token that we will add to the LLM (named \"<kee><i>\" where <kee> and <i> are replaced by the KEE's name and i will be replaced by the construction it appears in).\n",
    "        } for kee in tokenized_kees]\n",
    "        \n",
    "        json_comapp += out_json\n",
    "        csv_comapp += out_csv\n",
    "        \n",
    "with open(\"../../data/pseudowords/CoMaPP_all_bert.json\", \"w\") as file:\n",
    "    json.dump(json_comapp, file, ensure_ascii=False)\n",
    "\n",
    "f\"{errors} elements from {len(problematic_constructions)} different constructions could not be saved as intended.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd97868172d7e167"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"../../data/pseudowords/CoMapp_Dataset_bert.csv\", \"w+\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"label\", \"text\", \"pos_tags\", \"xpos_tags\", \"dep_rels\", \"dep_heads\", \"mask\", \"ambiguous_word\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_comapp)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b23c4e3f35e3966"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
