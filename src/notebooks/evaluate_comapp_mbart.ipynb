{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-27T17:05:36.474259Z"
    }
   },
   "id": "863338ddbf31a6c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../../data/pseudowords/CoMaPP_all.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data = [{\"example\": d[\"target1\"], \"cue\": \" \".join(d[\"target1\"].split()[:d[\"query_idx\"]]), \"pseudoword\": d[\"label\"]} for d in data]\n",
    "df = pd.DataFrame.from_dict(data).drop_duplicates(ignore_index=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "18bb06d46d72aab4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['index'] = df['pseudoword'].str.extract('(\\d+)').astype(int)\n",
    "df.set_index('index', inplace=True)\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b7e1f6e88f687297"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contextleft = pd.read_pickle(\"../../data/pseudowords/contextleft_text.pickle\")\n",
    "\n",
    "def update_cue(row):\n",
    "    output = row[['example', 'cue']]\n",
    "    if row['cue'] == '':  # if the string in cue is empty\n",
    "        # match the index of row with contextleft['construction_id'] and match contextleft['text'] with row['example'] and create matching_entry\n",
    "        matching_entry = contextleft.loc[(contextleft['construction_id'] == row.name) & (contextleft['text'] == row['example']), 'contextleft'].tolist()\n",
    "        if len(matching_entry) > 0:\n",
    "            output = [matching_entry[0] + \" \" + row['example'], matching_entry[0]]\n",
    "    return output\n",
    "\n",
    "# Add the left context if there is no cue up until the pseudoword.\n",
    "df[[\"example\", \"cue\"]] = df.apply(update_cue, axis=1)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a898840ef3c9c442"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'construction'}, inplace=True)\n",
    "\n",
    "result_df = df.groupby(['construction', 'pseudoword']).agg({'example': list, 'cue': list})\n",
    "\n",
    "result_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4a4e3d92707b4c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../../out/definitions.pickle\", \"rb\") as definitions_file:\n",
    "    definitions = pd.DataFrame.from_dict(pickle.load(definitions_file), orient=\"index\", columns=[\"definition\"])\n",
    "    \n",
    "definitions"
   ],
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "examples = pd.merge(result_df, definitions, how=\"inner\", left_on=\"construction\", right_index=True)\n",
    "examples"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "238d2238bbdd91d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generieren neuer Sätze:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bc899cb800665fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pseudowords = [\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_0_69.npy\"),\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_69_93.npy\"),\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_93_165.npy\"),\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_165_176.npy\"),\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_176_281.npy\"),\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_281_364.npy\"),\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_364_437.npy\"),\n",
    "    np.load(f\"../../data/pseudowords/mbart/pseudowords_comapp_437_562.npy\")\n",
    "]\n",
    "pseudowords = np.concatenate(pseudowords)\n",
    "pseudowords"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c9e6d940aa4aaace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_data = []\n",
    "# TODO mBART-Order laden\n",
    "for i in range(0, 8):\n",
    "    csv_data.append(pd.read_csv(f\"../../data/pseudowords/mbart/order_{i}.csv\", sep=\";\", index_col=0, header=None, quotechar=\"|\", names=[\"order\", \"label\"]))\n",
    "csv_data = pd.concat(csv_data)\n",
    "csv_data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "621718c51c9db794"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mbart_tokens = [d[0] for d in csv_data.values]\n",
    "\n",
    "mbart_tokens, len(mbart_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d55848226af1aa95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the vanilla mbart model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1fd4b9029d49e1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/mbart-large-50\", return_dict=True\n",
    ") \n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\n",
    "    \"facebook/mbart-large-50\", src_lang=\"de_DE\", tgt_lang=\"de_DE\"\n",
    ")\n",
    "model.model.encoder.embed_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e82730a36da5ce9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add to existing embeddings:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31cb60117910e817"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_embeddings = torch.cat((model.model.encoder.embed_tokens.weight, torch.tensor(pseudowords)), dim=0)\n",
    "model.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(combined_embeddings)\n",
    "model.model.encoder.embed_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "de2759094c5daff5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add to existing tokens:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36118990fc6027ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(mbart_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5e0c6c41fda55604"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "29a15c814c4d9e1e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Complete the cues:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b006ebc8aa40484"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def complete_cues(row):\n",
    "    try:\n",
    "        output_texts = []\n",
    "        scores = []\n",
    "        for cue, example in zip(row[\"cue\"], row[\"example\"]):\n",
    "            assert row[\"pseudoword\"].iloc[0] in mbart_tokens  # skip pseudoword embeddings that haven't been learned\n",
    "            \n",
    "            input_text = cue + \" \" + row[\"pseudoword\"].iloc[0] + \" <mask>\"\n",
    "            target_length = int(1.5 * len(example))  # allow double the length of the original sentence\n",
    "            \n",
    "            outputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "            outputs = model.generate(outputs[\"input_ids\"].to(\"cuda:0\"), max_length=target_length, num_return_sequences=1, num_beams=10, output_scores=True, return_dict_in_generate=True)\n",
    "            output_text = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "            score = torch.exp(outputs.sequences_scores)\n",
    "            output_texts += output_text\n",
    "            \n",
    "            scores.append(score)\n",
    "        print(\".\", end=\"\")\n",
    "        return pd.Series({\"construction\": row[\"construction\"], \"pseudoword\": row[\"pseudoword\"].iloc[0], \"orig_example\": row[\"example\"], \"generated\": str(output_texts), \"scores\": str([float(score) for score in scores])})\n",
    "    except Exception as e:\n",
    "        print(\":\", end=\"\")\n",
    "        return pd.Series({\"construction\": row[\"construction\"], \"pseudoword\": row[\"pseudoword\"].iloc[0], \"orig_example\": row[\"example\"], \"generated\": str(e), \"scores\": \"[-1.0]\"})\n",
    "\n",
    "examples_reset = examples.reset_index()\n",
    "pseudoword_output_scores = examples_reset[[\"construction\", \"pseudoword\", \"example\", \"cue\", \"pseudoword\"]].progress_apply(complete_cues, axis=1)\n",
    "pseudoword_output_scores"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b7fc546c4aeb79b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "examples = pseudoword_output_scores[[\"pseudoword\", \"generated\", \"scores\"]]\n",
    "\n",
    "examples"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "14e01733b5a4aeba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Anzahl neuer Sätze"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64a06083f0ccd35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "examples.to_csv(f\"../../out/comapp/mbart/data_mbart.tsv\", sep=\"\\t\", decimal=\",\")\n",
    "examples.to_excel(f\"../../out/comapp/mbart/data_mbart.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "93ee2263ebe468ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pseudoword_output_scores.to_csv(\"../../out/comapp/mbart/data_mbart_complete.tsv\", sep=\"\\t\", decimal=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a890185b466ac02e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
