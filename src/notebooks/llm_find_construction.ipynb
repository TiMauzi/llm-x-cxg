{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip3 install transformers==4.33.2\n",
    "!pip3 install optimum==1.13.2\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce1251cb4707678d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:51:48.043160800Z",
     "start_time": "2024-01-03T14:51:31.379610600Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name_or_path = 'TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ'  # \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map='auto',\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"gptq-4bit-32g-actorder_True\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7057c80cba5a4c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"../../out/definitions.pickle\", \"rb\") as file:\n",
    "    definitions = pickle.load(file)\n",
    "with open(\"../../out/sentences.pickle\", \"rb\") as file:\n",
    "    sentences = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:04:03.112119400Z",
     "start_time": "2024-01-03T15:04:03.015815400Z"
    }
   },
   "id": "d1160f2523ef3d41",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_examples(definition: str, options: dict, temperature=0.1, top_p=0.95, top_k=1, max_new_tokens=5):\n",
    "    examples = \"\"\n",
    "    for num, option in options.items():\n",
    "        examples += f\"{num}. {option} \"\n",
    "    prompt = lambda definition, examples: f'''### User: Du bist genau und gewissenhaft und gibst stets die korrekte Antwort. Hier ist eine Definition: {definition} Hier sind nummerierte Beispiele: {examples} Nenne die Nummer des Beispiels, das zur Definition passt.\n",
    "        ### Assistant: '''\n",
    "    prompt_length = len(prompt(definition, examples))\n",
    "    i = 100  # only try 100 times\n",
    "    output = -1\n",
    "    while (output >= 0) and (i > 0):\n",
    "        i -= 1\n",
    "        input_ids = tokenizer(prompt(definition, examples), return_tensors='pt').input_ids.cuda()\n",
    "        output = model.generate(inputs=input_ids, temperature=temperature,\n",
    "                                do_sample=True, top_p=top_p, top_k=top_k,\n",
    "                                max_new_tokens=max_new_tokens)\n",
    "        output = tokenizer.decode(output[0])[prompt_length:].strip()\n",
    "        output = re.findall('\\d+', output)\n",
    "        if len(output) > 0:\n",
    "            output = int(output)\n",
    "        print(i, end=\" \")\n",
    "        if i == 0:\n",
    "            return -1\n",
    "    print()\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f56312455ff2948"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/211 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5004569c1655411683c7bc8d337b660c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m others \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoices(\u001B[38;5;28mlist\u001B[39m(others), k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m9\u001B[39m)\n\u001B[0;32m      5\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(others) \u001B[38;5;241m|\u001B[39m {sentence}\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for false_positives in range(2, 6):\n",
    "    result = []\n",
    "    for key, definition in tqdm(definitions.items()):\n",
    "        sentence = random.choice(list(sentences[int(key)]))\n",
    "        others = itertools.chain.from_iterable([sentence_list for constr, sentence_list in sentences.items() if int(constr) != int(key)])\n",
    "        others = random.choices(list(others), k=false_positives)\n",
    "        query = dict(enumerate(set(others) | {sentence}))\n",
    "        prediction = find_examples(definition, query)\n",
    "        print(query[prediction] == sentence, sentence, prediction)\n",
    "        result.append(pd.Series({\"constr\": key, \"definition\": definition, \"example\": sentence, \"prediction\": prediction, \"correct\": query[prediction] == sentence}))\n",
    "    result = pd.DataFrame(result)\n",
    "    result.to_csv(f\"result_1_in_{false_positives}.tsv\", sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:20:54.759995500Z",
     "start_time": "2024-01-03T15:09:18.609718600Z"
    }
   },
   "id": "de55757390ccac1e",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
